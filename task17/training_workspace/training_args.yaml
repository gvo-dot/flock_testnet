- Qwen/Qwen1.5-0.5B:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 8
    num_train_epochs: 1
    lora_rank: 8
    lora_alpha: 16
    lora_dropout: 0.1
    learning_rate: 1.0e-4
    warmup_steps: 20
- Qwen/Qwen1.5-1.8B:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 8
    num_train_epochs: 1
    lora_rank: 8
    lora_alpha: 16
    lora_dropout: 0.1
    learning_rate: 1.0e-4
    warmup_steps: 20
- google/gemma-2b:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 8
    num_train_epochs: 1
    lora_rank: 8
    lora_alpha: 16
    lora_dropout: 0.05
    learning_rate: 1.0e-4
    warmup_steps: 20
