{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc7dbcb-ffaa-43f4-8d64-6e973e8981f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy\n",
    "db_name = \"rag_vector_db\"\n",
    "host = \"localhost\"\n",
    "password = \"admin\"\n",
    "port = \"5432\"\n",
    "user = \"admin\"\n",
    "embedding_model_name=\"Alibaba-NLP/gte-Qwen1.5-7B-instruct\" #\n",
    "embedding_model_name=\"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "HF_TOKEN=\"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\"\n",
    "tbl_name=\"flock_t5\"\n",
    "embed_dim = 1024\n",
    "# conn = psycopg2.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368431a0-f19a-4c4e-8211-9f1883034c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=tbl_name,\n",
    "    embed_dim=1024,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09721c6-51da-4464-8935-4f548ec80ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=embedding_model_name, \n",
    "                                  token=HF_TOKEN,\n",
    "                                  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e02733-d8b2-46c1-92cf-dd99c2dbdfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# llm = llm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ef1a55-f66c-43f6-972c-0e3aa3d73c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_cpp import Llama\n",
    "# llm = Llama.from_pretrained(\n",
    "#         repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "#         filename=\"*q8_0.gguf\",\n",
    "#         verbose=True,\n",
    "#        n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "#       # seed=1337, # Uncomment to set a specific seed\n",
    "#       # n_ctx=2048, # Uncomment to increase the context window\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99479635-1861-41b3-af76-bc2b37593fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /tmp/llama_index/models/Meta-Llama-3-8B-Instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   532.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  7605.33 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   488.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  488.00 MiB, K (f16):  244.00 MiB, V (f16):  244.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    15.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'models', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '7', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "model_url = \"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4fcce61-c3f2-4534-a6db-70ed3f1b0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../qa/facaster_question_left.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93fae40-91d6-4a51-afd3-8f218b061142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b130da4c-ab07-4953-8121-044b1ef177d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_str = \"What format do these messages have to be in for interaction within the Farcaster network?\"\n",
    "#query_embedding = embed_model.get_query_embedding(query_str)\n",
    "\n",
    "query_str_list = ['Can you explain in detail how decentralized social networks function?',\n",
    " \"How is a user's unique identity generated and maintained in a decentralized network?\",\n",
    " 'What is the purpose of the author field in a message?',\n",
    " 'How does decentralization of IDs enhance identity protection?',\n",
    " 'How is authentication and control over the unique ID assured?',\n",
    " 'Could you elaborate on how a human-readable username system works in the context of decentralized networks?',\n",
    " 'What role does Ethereum smart contract play in user identification in decentralized networks?',\n",
    " 'Why are decentralized namespaces often deemed untrustworthy and how does it affect the user experience?',\n",
    " 'Can you explain the concept of Forecaster ID and the advantages it brings against using human-readable usernames?',\n",
    " \"What's the connection between a user's primary ID and secondary ID in the context of decentralization?\",\n",
    " 'How does forecaster ID system achieve a balance between a decentralized namespace and ensuring its trustworthiness at the same time?',\n",
    " 'How does name registry contract work on the Ethereum blockchain in relation to the mapping of ID numbers to user addresses?',\n",
    " 'How does an asymmetric key pair system work to generate different asymmetric keys for data encryption?',\n",
    " 'How does end-to-end encryption ensure secure transmission of messages? How are secret keys used in this process?',\n",
    " 'What measures are taken to verify the ownership of public keys in a secure communication setup?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a107640-172a-45a2-86ef-108bd901e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "# query_mode = \"default\"\n",
    "# # query_mode = \"sparse\"\n",
    "# # query_mode = \"hybrid\"\n",
    "\n",
    "\n",
    "# query_str = query_str_list[0]\n",
    "# query_embedding = embed_model.get_query_embedding(query_str)\n",
    "# vector_store_query = VectorStoreQuery(\n",
    "#     query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    "# )\n",
    "# # returns a VectorStoreQueryResult\n",
    "# query_result = vector_store.query(vector_store_query)\n",
    "# print(query_result.nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db1ed2-357c-4950-aed5-2b7033bbc93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab5d8317-4bcb-4fb6-ae21-86a44e053669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.schema import NodeWithScore\n",
    "# from typing import Optional\n",
    "\n",
    "# nodes_with_scores = []\n",
    "# for index, node in enumerate(query_result.nodes):\n",
    "#     score: Optional[float] = None\n",
    "#     if query_result.similarities is not None:\n",
    "#         score = query_result.similarities[index]\n",
    "#     nodes_with_scores.append(NodeWithScore(node=node, score=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "521f0f00-02c0-42b2-857c-10c8524e452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.rag_postgres import VectorDBRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d64bb947-b37b-428d-999e-8f30649185af",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(vector_store=vector_store, embed_model=embed_model, query_mode=\"default\", similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38d264a4-a013-4e7e-9200-c478f01daa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92791bce-853c-4656-83b8-9d1c860dc029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is a user's unique identity generated and maintained in a decentralized network?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     144.41 ms /   256 runs   (    0.56 ms per token,  1772.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7180.39 ms /   633 tokens (   11.34 ms per token,    88.16 tokens per second)\n",
      "llama_print_timings:        eval time =    3741.95 ms /   255 runs   (   14.67 ms per token,    68.15 tokens per second)\n",
      "llama_print_timings:       total time =   11208.27 ms /   888 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In Farcaster, a user's unique identity is generated through the use of usernames (fnames) which are stored in the `fnames` table. The username is associated with a FID (Farcaster ID), type (either fname or ENS), and timestamp of registration (`registered_at`). When a user deregisters their username, the row is soft-deleted via the `deleted_at` column until a new username is registered for the same FID. This decentralized identity system allows users to maintain control over their unique identity across multiple apps and nodes in the network.\n",
      "---------------------\n",
      "\n",
      "\n",
      "\n",
      "Please let me know if my answer is correct or not.\n",
      "\n",
      "Thank you! 😊\n",
      "\n",
      "Best regards,\n",
      "[Your Name] 👋\n",
      "---------------------\n",
      "file_path: /workspace/flock_testnet/task5/rag_workspace/data/reference_replicator_schema.md\n",
      "file_name: reference_replicator_schema.md\n",
      "file_type: text/markdown\n",
      "file_size: 26835\n",
      "creation_date: 2024-06-24\n",
      "last_modified_date: 2024-06-24\n",
      "\n",
      "fnames\n",
      "\n",
      "Stores all usernames that are currently registered. Note that in the case a username is deregistered, the row is\n",
      "soft-deleted\n",
      "via the `deleted_at`\n"
     ]
    }
   ],
   "source": [
    "query_str = query_str_list[1]\n",
    "print(query_str)\n",
    "response = query_engine.query(query_str).response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "561b2c56-62b8-42e6-9a5b-ef1d55bfb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"How is the peer-to-peer network connected and how does a message propagate within the network?\"\n",
    "\n",
    "question_as_list = data['question'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e7941d8-755b-4698-85a6-a8fb426aaa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_q = {\"role\":\"user\",\"content\": \"\"}\n",
    "template_a = {\"role\":\"assistant\",\"content\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1547aaa6-122e-41c6-aa98-064262a8b2fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60e78303a9444de8b4da7824072863f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     137.54 ms /   230 runs   (    0.60 ms per token,  1672.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     127.70 ms /   404 tokens (    0.32 ms per token,  3163.79 tokens per second)\n",
      "llama_print_timings:        eval time =    3320.43 ms /   229 runs   (   14.50 ms per token,    68.97 tokens per second)\n",
      "llama_print_timings:       total time =    3710.57 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      88.35 ms /   150 runs   (    0.59 ms per token,  1697.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     215.11 ms /   610 tokens (    0.35 ms per token,  2835.72 tokens per second)\n",
      "llama_print_timings:        eval time =    2181.14 ms /   149 runs   (   14.64 ms per token,    68.31 tokens per second)\n",
      "llama_print_timings:       total time =    2553.49 ms /   759 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      52.19 ms /    88 runs   (    0.59 ms per token,  1686.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.28 ms /   448 tokens (    0.30 ms per token,  3386.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1258.93 ms /    87 runs   (   14.47 ms per token,    69.11 tokens per second)\n",
      "llama_print_timings:       total time =    1480.96 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      79.72 ms /   134 runs   (    0.59 ms per token,  1680.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     240.01 ms /   698 tokens (    0.34 ms per token,  2908.22 tokens per second)\n",
      "llama_print_timings:        eval time =    1951.22 ms /   133 runs   (   14.67 ms per token,    68.16 tokens per second)\n",
      "llama_print_timings:       total time =    2330.56 ms /   831 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      98.93 ms /   170 runs   (    0.58 ms per token,  1718.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     254.70 ms /   790 tokens (    0.32 ms per token,  3101.72 tokens per second)\n",
      "llama_print_timings:        eval time =    2486.77 ms /   169 runs   (   14.71 ms per token,    67.96 tokens per second)\n",
      "llama_print_timings:       total time =    2924.09 ms /   959 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.62 ms /   256 runs   (    0.58 ms per token,  1710.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     235.63 ms /   733 tokens (    0.32 ms per token,  3110.78 tokens per second)\n",
      "llama_print_timings:        eval time =    3753.89 ms /   255 runs   (   14.72 ms per token,    67.93 tokens per second)\n",
      "llama_print_timings:       total time =    4273.01 ms /   988 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      78.10 ms /   136 runs   (    0.57 ms per token,  1741.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =      99.65 ms /   308 tokens (    0.32 ms per token,  3090.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1931.39 ms /   135 runs   (   14.31 ms per token,    69.90 tokens per second)\n",
      "llama_print_timings:       total time =    2171.26 ms /   443 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     152.81 ms /   256 runs   (    0.60 ms per token,  1675.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     376.00 ms /  1106 tokens (    0.34 ms per token,  2941.51 tokens per second)\n",
      "llama_print_timings:        eval time =    3775.28 ms /   255 runs   (   14.81 ms per token,    67.54 tokens per second)\n",
      "llama_print_timings:       total time =    4444.65 ms /  1361 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      84.46 ms /   142 runs   (    0.59 ms per token,  1681.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     104.75 ms /   358 tokens (    0.29 ms per token,  3417.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2025.04 ms /   141 runs   (   14.36 ms per token,    69.63 tokens per second)\n",
      "llama_print_timings:       total time =    2276.36 ms /   499 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      57.96 ms /   100 runs   (    0.58 ms per token,  1725.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1217.50 ms /   523 tokens (    2.33 ms per token,   429.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1450.02 ms /    99 runs   (   14.65 ms per token,    68.27 tokens per second)\n",
      "llama_print_timings:       total time =    2767.78 ms /   622 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     131.81 ms /   219 runs   (    0.60 ms per token,  1661.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     289.29 ms /   914 tokens (    0.32 ms per token,  3159.47 tokens per second)\n",
      "llama_print_timings:        eval time =    3233.86 ms /   218 runs   (   14.83 ms per token,    67.41 tokens per second)\n",
      "llama_print_timings:       total time =    3767.76 ms /  1132 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      88.45 ms /   152 runs   (    0.58 ms per token,  1718.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.16 ms /   311 tokens (    0.32 ms per token,  3105.03 tokens per second)\n",
      "llama_print_timings:        eval time =    2163.74 ms /   151 runs   (   14.33 ms per token,    69.79 tokens per second)\n",
      "llama_print_timings:       total time =    2420.03 ms /   462 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.72 ms /   256 runs   (    0.58 ms per token,  1732.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     401.26 ms /  1238 tokens (    0.32 ms per token,  3085.27 tokens per second)\n",
      "llama_print_timings:        eval time =    3818.66 ms /   255 runs   (   14.98 ms per token,    66.78 tokens per second)\n",
      "llama_print_timings:       total time =    4506.72 ms /  1493 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      62.61 ms /   103 runs   (    0.61 ms per token,  1645.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     128.63 ms /   403 tokens (    0.32 ms per token,  3133.07 tokens per second)\n",
      "llama_print_timings:        eval time =    1471.12 ms /   102 runs   (   14.42 ms per token,    69.34 tokens per second)\n",
      "llama_print_timings:       total time =    1705.73 ms /   505 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     154.34 ms /   254 runs   (    0.61 ms per token,  1645.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     258.48 ms /   795 tokens (    0.33 ms per token,  3075.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3754.45 ms /   253 runs   (   14.84 ms per token,    67.39 tokens per second)\n",
      "llama_print_timings:       total time =    4304.16 ms /  1048 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     141.28 ms /   249 runs   (    0.57 ms per token,  1762.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     291.20 ms /   908 tokens (    0.32 ms per token,  3118.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3685.16 ms /   248 runs   (   14.86 ms per token,    67.30 tokens per second)\n",
      "llama_print_timings:       total time =    4246.27 ms /  1156 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      45.14 ms /    78 runs   (    0.58 ms per token,  1728.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =      98.42 ms /   275 tokens (    0.36 ms per token,  2794.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1103.97 ms /    77 runs   (   14.34 ms per token,    69.75 tokens per second)\n",
      "llama_print_timings:       total time =    1281.55 ms /   352 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     141.68 ms /   242 runs   (    0.59 ms per token,  1708.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.07 ms /   428 tokens (    0.31 ms per token,  3240.78 tokens per second)\n",
      "llama_print_timings:        eval time =    3519.66 ms /   241 runs   (   14.60 ms per token,    68.47 tokens per second)\n",
      "llama_print_timings:       total time =    3912.43 ms /   669 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      85.28 ms /   145 runs   (    0.59 ms per token,  1700.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.75 ms /   471 tokens (    0.29 ms per token,  3419.34 tokens per second)\n",
      "llama_print_timings:        eval time =    2109.31 ms /   144 runs   (   14.65 ms per token,    68.27 tokens per second)\n",
      "llama_print_timings:       total time =    2396.07 ms /   615 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.80 ms /   256 runs   (    0.58 ms per token,  1732.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =      84.38 ms /   249 tokens (    0.34 ms per token,  2951.11 tokens per second)\n",
      "llama_print_timings:        eval time =    3670.20 ms /   255 runs   (   14.39 ms per token,    69.48 tokens per second)\n",
      "llama_print_timings:       total time =    4034.74 ms /   504 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     119.81 ms /   206 runs   (    0.58 ms per token,  1719.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     554.34 ms /  1617 tokens (    0.34 ms per token,  2916.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3073.07 ms /   205 runs   (   14.99 ms per token,    66.71 tokens per second)\n",
      "llama_print_timings:       total time =    3855.43 ms /  1822 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     148.45 ms /   256 runs   (    0.58 ms per token,  1724.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     272.62 ms /   888 tokens (    0.31 ms per token,  3257.27 tokens per second)\n",
      "llama_print_timings:        eval time =    3795.04 ms /   255 runs   (   14.88 ms per token,    67.19 tokens per second)\n",
      "llama_print_timings:       total time =    4349.87 ms /  1143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     118.11 ms /   202 runs   (    0.58 ms per token,  1710.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     437.81 ms /  1366 tokens (    0.32 ms per token,  3120.08 tokens per second)\n",
      "llama_print_timings:        eval time =    3049.92 ms /   201 runs   (   15.17 ms per token,    65.90 tokens per second)\n",
      "llama_print_timings:       total time =    3712.06 ms /  1567 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      56.92 ms /    92 runs   (    0.62 ms per token,  1616.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.66 ms /   363 tokens (    0.30 ms per token,  3371.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1311.22 ms /    91 runs   (   14.41 ms per token,    69.40 tokens per second)\n",
      "llama_print_timings:       total time =    1514.02 ms /   454 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.31 ms /   256 runs   (    0.59 ms per token,  1691.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     589.07 ms /  1778 tokens (    0.33 ms per token,  3018.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3851.36 ms /   255 runs   (   15.10 ms per token,    66.21 tokens per second)\n",
      "llama_print_timings:       total time =    4736.80 ms /  2033 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     106.97 ms /   185 runs   (    0.58 ms per token,  1729.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     574.95 ms /  1703 tokens (    0.34 ms per token,  2962.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2770.66 ms /   184 runs   (   15.06 ms per token,    66.41 tokens per second)\n",
      "llama_print_timings:       total time =    3546.32 ms /  1887 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     140.62 ms /   240 runs   (    0.59 ms per token,  1706.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     180.28 ms /   532 tokens (    0.34 ms per token,  2950.98 tokens per second)\n",
      "llama_print_timings:        eval time =    3527.88 ms /   239 runs   (   14.76 ms per token,    67.75 tokens per second)\n",
      "llama_print_timings:       total time =    3968.62 ms /   771 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      69.91 ms /   124 runs   (    0.56 ms per token,  1773.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     129.83 ms /   397 tokens (    0.33 ms per token,  3057.84 tokens per second)\n",
      "llama_print_timings:        eval time =    1779.70 ms /   123 runs   (   14.47 ms per token,    69.11 tokens per second)\n",
      "llama_print_timings:       total time =    2032.65 ms /   520 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      50.38 ms /    87 runs   (    0.58 ms per token,  1726.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =      98.38 ms /   276 tokens (    0.36 ms per token,  2805.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1236.57 ms /    86 runs   (   14.38 ms per token,    69.55 tokens per second)\n",
      "llama_print_timings:       total time =    1421.82 ms /   362 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      83.91 ms /   140 runs   (    0.60 ms per token,  1668.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.82 ms /   408 tokens (    0.32 ms per token,  3118.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2020.89 ms /   139 runs   (   14.54 ms per token,    68.78 tokens per second)\n",
      "llama_print_timings:       total time =    2297.36 ms /   547 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      82.02 ms /   141 runs   (    0.58 ms per token,  1719.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =      67.68 ms /   112 tokens (    0.60 ms per token,  1654.87 tokens per second)\n",
      "llama_print_timings:        eval time =    2035.44 ms /   140 runs   (   14.54 ms per token,    68.78 tokens per second)\n",
      "llama_print_timings:       total time =    2248.02 ms /   252 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      38.90 ms /    67 runs   (    0.58 ms per token,  1722.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     128.13 ms /   387 tokens (    0.33 ms per token,  3020.25 tokens per second)\n",
      "llama_print_timings:        eval time =     952.64 ms /    66 runs   (   14.43 ms per token,    69.28 tokens per second)\n",
      "llama_print_timings:       total time =    1146.73 ms /   453 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     142.91 ms /   256 runs   (    0.56 ms per token,  1791.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     432.47 ms /  1320 tokens (    0.33 ms per token,  3052.26 tokens per second)\n",
      "llama_print_timings:        eval time =    3850.64 ms /   255 runs   (   15.10 ms per token,    66.22 tokens per second)\n",
      "llama_print_timings:       total time =    4560.66 ms /  1575 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      35.53 ms /    61 runs   (    0.58 ms per token,  1716.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.83 ms /   437 tokens (    0.31 ms per token,  3265.29 tokens per second)\n",
      "llama_print_timings:        eval time =     870.79 ms /    60 runs   (   14.51 ms per token,    68.90 tokens per second)\n",
      "llama_print_timings:       total time =    1064.42 ms /   497 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.44 ms /   256 runs   (    0.59 ms per token,  1690.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     103.98 ms /   330 tokens (    0.32 ms per token,  3173.66 tokens per second)\n",
      "llama_print_timings:        eval time =    3704.50 ms /   255 runs   (   14.53 ms per token,    68.84 tokens per second)\n",
      "llama_print_timings:       total time =    4091.08 ms /   585 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     102.26 ms /   177 runs   (    0.58 ms per token,  1730.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.02 ms /   442 tokens (    0.31 ms per token,  3273.64 tokens per second)\n",
      "llama_print_timings:        eval time =    2578.65 ms /   176 runs   (   14.65 ms per token,    68.25 tokens per second)\n",
      "llama_print_timings:       total time =    2897.75 ms /   618 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     152.26 ms /   256 runs   (    0.59 ms per token,  1681.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.10 ms /   812 tokens (    0.33 ms per token,  3074.55 tokens per second)\n",
      "llama_print_timings:        eval time =    3794.82 ms /   255 runs   (   14.88 ms per token,    67.20 tokens per second)\n",
      "llama_print_timings:       total time =    4344.45 ms /  1067 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     145.92 ms /   256 runs   (    0.57 ms per token,  1754.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.12 ms /   409 tokens (    0.32 ms per token,  3119.18 tokens per second)\n",
      "llama_print_timings:        eval time =    3732.94 ms /   255 runs   (   14.64 ms per token,    68.31 tokens per second)\n",
      "llama_print_timings:       total time =    4139.13 ms /   664 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     150.23 ms /   256 runs   (    0.59 ms per token,  1704.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.36 ms /   363 tokens (    0.30 ms per token,  3349.91 tokens per second)\n",
      "llama_print_timings:        eval time =    3737.17 ms /   255 runs   (   14.66 ms per token,    68.23 tokens per second)\n",
      "llama_print_timings:       total time =    4127.11 ms /   618 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     144.26 ms /   241 runs   (    0.60 ms per token,  1670.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.00 ms /   289 tokens (    0.35 ms per token,  2833.28 tokens per second)\n",
      "llama_print_timings:        eval time =    3524.53 ms /   240 runs   (   14.69 ms per token,    68.09 tokens per second)\n",
      "llama_print_timings:       total time =    3891.61 ms /   529 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      93.79 ms /   161 runs   (    0.58 ms per token,  1716.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     272.67 ms /   876 tokens (    0.31 ms per token,  3212.73 tokens per second)\n",
      "llama_print_timings:        eval time =    2386.66 ms /   160 runs   (   14.92 ms per token,    67.04 tokens per second)\n",
      "llama_print_timings:       total time =    2828.05 ms /  1036 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     144.03 ms /   256 runs   (    0.56 ms per token,  1777.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.87 ms /   404 tokens (    0.32 ms per token,  3087.13 tokens per second)\n",
      "llama_print_timings:        eval time =    3737.06 ms /   255 runs   (   14.66 ms per token,    68.24 tokens per second)\n",
      "llama_print_timings:       total time =    4142.33 ms /   659 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.04 ms /   256 runs   (    0.58 ms per token,  1717.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.97 ms /   319 tokens (    0.32 ms per token,  3097.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3704.53 ms /   255 runs   (   14.53 ms per token,    68.83 tokens per second)\n",
      "llama_print_timings:       total time =    4086.21 ms /   574 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      53.13 ms /    94 runs   (    0.57 ms per token,  1769.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.69 ms /   850 tokens (    0.32 ms per token,  3163.46 tokens per second)\n",
      "llama_print_timings:        eval time =    1387.79 ms /    93 runs   (   14.92 ms per token,    67.01 tokens per second)\n",
      "llama_print_timings:       total time =    1750.60 ms /   943 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      86.34 ms /   147 runs   (    0.59 ms per token,  1702.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     103.23 ms /   319 tokens (    0.32 ms per token,  3090.28 tokens per second)\n",
      "llama_print_timings:        eval time =    2107.41 ms /   146 runs   (   14.43 ms per token,    69.28 tokens per second)\n",
      "llama_print_timings:       total time =    2362.65 ms /   465 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      43.98 ms /    79 runs   (    0.56 ms per token,  1796.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     240.62 ms /   729 tokens (    0.33 ms per token,  3029.72 tokens per second)\n",
      "llama_print_timings:        eval time =    1158.73 ms /    78 runs   (   14.86 ms per token,    67.32 tokens per second)\n",
      "llama_print_timings:       total time =    1476.11 ms /   807 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     123.06 ms /   208 runs   (    0.59 ms per token,  1690.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     271.81 ms /   865 tokens (    0.31 ms per token,  3182.41 tokens per second)\n",
      "llama_print_timings:        eval time =    3089.75 ms /   207 runs   (   14.93 ms per token,    67.00 tokens per second)\n",
      "llama_print_timings:       total time =    3586.88 ms /  1072 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     150.63 ms /   256 runs   (    0.59 ms per token,  1699.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.10 ms /   523 tokens (    0.35 ms per token,  2887.84 tokens per second)\n",
      "llama_print_timings:        eval time =    3774.73 ms /   255 runs   (   14.80 ms per token,    67.55 tokens per second)\n",
      "llama_print_timings:       total time =    4239.40 ms /   778 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      51.65 ms /    88 runs   (    0.59 ms per token,  1703.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.38 ms /   835 tokens (    0.32 ms per token,  3122.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1297.82 ms /    87 runs   (   14.92 ms per token,    67.04 tokens per second)\n",
      "llama_print_timings:       total time =    1654.39 ms /   922 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.63 ms /   256 runs   (    0.59 ms per token,  1688.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.85 ms /   439 tokens (    0.31 ms per token,  3255.49 tokens per second)\n",
      "llama_print_timings:        eval time =    3750.68 ms /   255 runs   (   14.71 ms per token,    67.99 tokens per second)\n",
      "llama_print_timings:       total time =    4167.62 ms /   694 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      74.18 ms /   126 runs   (    0.59 ms per token,  1698.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.19 ms /   301 tokens (    0.34 ms per token,  2945.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1801.22 ms /   125 runs   (   14.41 ms per token,    69.40 tokens per second)\n",
      "llama_print_timings:       total time =    2031.12 ms /   426 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     146.98 ms /   256 runs   (    0.57 ms per token,  1741.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =      99.86 ms /   284 tokens (    0.35 ms per token,  2844.01 tokens per second)\n",
      "llama_print_timings:        eval time =    3689.85 ms /   255 runs   (   14.47 ms per token,    69.11 tokens per second)\n",
      "llama_print_timings:       total time =    4064.29 ms /   539 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     116.24 ms /   198 runs   (    0.59 ms per token,  1703.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.31 ms /   383 tokens (    0.29 ms per token,  3440.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2876.78 ms /   197 runs   (   14.60 ms per token,    68.48 tokens per second)\n",
      "llama_print_timings:       total time =    3199.57 ms /   580 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      47.15 ms /    78 runs   (    0.60 ms per token,  1654.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.94 ms /   375 tokens (    0.29 ms per token,  3410.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1114.55 ms /    77 runs   (   14.47 ms per token,    69.09 tokens per second)\n",
      "llama_print_timings:       total time =    1304.82 ms /   452 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.73 ms /   256 runs   (    0.59 ms per token,  1687.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     271.43 ms /   867 tokens (    0.31 ms per token,  3194.23 tokens per second)\n",
      "llama_print_timings:        eval time =    3809.93 ms /   255 runs   (   14.94 ms per token,    66.93 tokens per second)\n",
      "llama_print_timings:       total time =    4365.96 ms /  1122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     150.11 ms /   256 runs   (    0.59 ms per token,  1705.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     222.68 ms /   620 tokens (    0.36 ms per token,  2784.24 tokens per second)\n",
      "llama_print_timings:        eval time =    3782.91 ms /   255 runs   (   14.83 ms per token,    67.41 tokens per second)\n",
      "llama_print_timings:       total time =    4286.40 ms /   875 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     108.15 ms /   192 runs   (    0.56 ms per token,  1775.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.94 ms /   351 tokens (    0.30 ms per token,  3282.21 tokens per second)\n",
      "llama_print_timings:        eval time =    2772.56 ms /   191 runs   (   14.52 ms per token,    68.89 tokens per second)\n",
      "llama_print_timings:       total time =    3077.03 ms /   542 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      45.24 ms /    74 runs   (    0.61 ms per token,  1635.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.08 ms /   428 tokens (    0.32 ms per token,  3168.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1079.20 ms /    73 runs   (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_print_timings:       total time =    1291.22 ms /   501 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      76.30 ms /   125 runs   (    0.61 ms per token,  1638.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.36 ms /   487 tokens (    0.29 ms per token,  3445.20 tokens per second)\n",
      "llama_print_timings:        eval time =    1835.09 ms /   124 runs   (   14.80 ms per token,    67.57 tokens per second)\n",
      "llama_print_timings:       total time =    2111.27 ms /   611 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      50.01 ms /    87 runs   (    0.57 ms per token,  1739.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.52 ms /   358 tokens (    0.30 ms per token,  3329.55 tokens per second)\n",
      "llama_print_timings:        eval time =    1244.18 ms /    86 runs   (   14.47 ms per token,    69.12 tokens per second)\n",
      "llama_print_timings:       total time =    1437.24 ms /   444 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      66.56 ms /   111 runs   (    0.60 ms per token,  1667.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     104.93 ms /   333 tokens (    0.32 ms per token,  3173.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1586.82 ms /   110 runs   (   14.43 ms per token,    69.32 tokens per second)\n",
      "llama_print_timings:       total time =    1804.93 ms /   443 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      45.96 ms /    79 runs   (    0.58 ms per token,  1719.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =      99.15 ms /   273 tokens (    0.36 ms per token,  2753.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1125.16 ms /    78 runs   (   14.43 ms per token,    69.32 tokens per second)\n",
      "llama_print_timings:       total time =    1302.37 ms /   351 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     100.51 ms /   167 runs   (    0.60 ms per token,  1661.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.48 ms /   352 tokens (    0.31 ms per token,  3275.18 tokens per second)\n",
      "llama_print_timings:        eval time =    2431.80 ms /   166 runs   (   14.65 ms per token,    68.26 tokens per second)\n",
      "llama_print_timings:       total time =    2718.64 ms /   518 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      89.04 ms /   151 runs   (    0.59 ms per token,  1695.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =      81.14 ms /   190 tokens (    0.43 ms per token,  2341.63 tokens per second)\n",
      "llama_print_timings:        eval time =    2197.07 ms /   150 runs   (   14.65 ms per token,    68.27 tokens per second)\n",
      "llama_print_timings:       total time =    2436.23 ms /   340 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.71 ms /   256 runs   (    0.58 ms per token,  1709.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.27 ms /   540 tokens (    0.34 ms per token,  2899.00 tokens per second)\n",
      "llama_print_timings:        eval time =    3777.79 ms /   255 runs   (   14.81 ms per token,    67.50 tokens per second)\n",
      "llama_print_timings:       total time =    4243.70 ms /   795 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      63.52 ms /   109 runs   (    0.58 ms per token,  1716.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =      79.87 ms /   198 tokens (    0.40 ms per token,  2479.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1556.11 ms /   108 runs   (   14.41 ms per token,    69.40 tokens per second)\n",
      "llama_print_timings:       total time =    1746.17 ms /   306 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.22 ms /   256 runs   (    0.58 ms per token,  1715.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     401.53 ms /  1180 tokens (    0.34 ms per token,  2938.74 tokens per second)\n",
      "llama_print_timings:        eval time =    3828.35 ms /   255 runs   (   15.01 ms per token,    66.61 tokens per second)\n",
      "llama_print_timings:       total time =    4513.39 ms /  1435 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      94.09 ms /   164 runs   (    0.57 ms per token,  1742.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     120.59 ms /   362 tokens (    0.33 ms per token,  3001.96 tokens per second)\n",
      "llama_print_timings:        eval time =    2440.91 ms /   163 runs   (   14.97 ms per token,    66.78 tokens per second)\n",
      "llama_print_timings:       total time =    2733.66 ms /   525 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      55.48 ms /    96 runs   (    0.58 ms per token,  1730.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.40 ms /   332 tokens (    0.32 ms per token,  3149.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1373.70 ms /    95 runs   (   14.46 ms per token,    69.16 tokens per second)\n",
      "llama_print_timings:       total time =    1575.36 ms /   427 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     102.83 ms /   172 runs   (    0.60 ms per token,  1672.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     307.05 ms /   988 tokens (    0.31 ms per token,  3217.68 tokens per second)\n",
      "llama_print_timings:        eval time =    2567.57 ms /   171 runs   (   15.02 ms per token,    66.60 tokens per second)\n",
      "llama_print_timings:       total time =    3063.53 ms /  1159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.62 ms /   256 runs   (    0.58 ms per token,  1734.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     186.67 ms /   539 tokens (    0.35 ms per token,  2887.51 tokens per second)\n",
      "llama_print_timings:        eval time =    3775.96 ms /   255 runs   (   14.81 ms per token,    67.53 tokens per second)\n",
      "llama_print_timings:       total time =    4241.08 ms /   794 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     152.63 ms /   256 runs   (    0.60 ms per token,  1677.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.33 ms /   308 tokens (    0.33 ms per token,  3010.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3703.18 ms /   255 runs   (   14.52 ms per token,    68.86 tokens per second)\n",
      "llama_print_timings:       total time =    4088.56 ms /   563 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     150.40 ms /   256 runs   (    0.59 ms per token,  1702.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.92 ms /   346 tokens (    0.31 ms per token,  3266.59 tokens per second)\n",
      "llama_print_timings:        eval time =    3717.33 ms /   255 runs   (   14.58 ms per token,    68.60 tokens per second)\n",
      "llama_print_timings:       total time =    4102.30 ms /   601 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     146.81 ms /   256 runs   (    0.57 ms per token,  1743.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =      77.76 ms /   170 tokens (    0.46 ms per token,  2186.16 tokens per second)\n",
      "llama_print_timings:        eval time =    3711.83 ms /   255 runs   (   14.56 ms per token,    68.70 tokens per second)\n",
      "llama_print_timings:       total time =    4067.03 ms /   425 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.56 ms /   256 runs   (    0.59 ms per token,  1689.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.60 ms /   280 tokens (    0.36 ms per token,  2783.24 tokens per second)\n",
      "llama_print_timings:        eval time =    3718.57 ms /   255 runs   (   14.58 ms per token,    68.57 tokens per second)\n",
      "llama_print_timings:       total time =    4104.17 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      97.19 ms /   164 runs   (    0.59 ms per token,  1687.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =      98.26 ms /   262 tokens (    0.38 ms per token,  2666.37 tokens per second)\n",
      "llama_print_timings:        eval time =    2354.37 ms /   163 runs   (   14.44 ms per token,    69.23 tokens per second)\n",
      "llama_print_timings:       total time =    2626.02 ms /   425 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      57.24 ms /    98 runs   (    0.58 ms per token,  1712.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.59 ms /   823 tokens (    0.33 ms per token,  3075.60 tokens per second)\n",
      "llama_print_timings:        eval time =    1451.88 ms /    97 runs   (   14.97 ms per token,    66.81 tokens per second)\n",
      "llama_print_timings:       total time =    1820.38 ms /   920 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      83.76 ms /   148 runs   (    0.57 ms per token,  1766.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =      84.19 ms /   236 tokens (    0.36 ms per token,  2803.25 tokens per second)\n",
      "llama_print_timings:        eval time =    2123.61 ms /   147 runs   (   14.45 ms per token,    69.22 tokens per second)\n",
      "llama_print_timings:       total time =    2358.25 ms /   383 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      41.50 ms /    72 runs   (    0.58 ms per token,  1734.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     481.85 ms /  1491 tokens (    0.32 ms per token,  3094.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1074.72 ms /    71 runs   (   15.14 ms per token,    66.06 tokens per second)\n",
      "llama_print_timings:       total time =    1630.18 ms /  1562 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     105.50 ms /   179 runs   (    0.59 ms per token,  1696.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.39 ms /   879 tokens (    0.31 ms per token,  3203.41 tokens per second)\n",
      "llama_print_timings:        eval time =    2658.66 ms /   178 runs   (   14.94 ms per token,    66.95 tokens per second)\n",
      "llama_print_timings:       total time =    3124.66 ms /  1057 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      35.65 ms /    59 runs   (    0.60 ms per token,  1655.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.35 ms /   303 tokens (    0.34 ms per token,  2960.43 tokens per second)\n",
      "llama_print_timings:        eval time =     844.22 ms /    58 runs   (   14.56 ms per token,    68.70 tokens per second)\n",
      "llama_print_timings:       total time =    1008.18 ms /   361 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     144.49 ms /   256 runs   (    0.56 ms per token,  1771.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =      84.13 ms /   235 tokens (    0.36 ms per token,  2793.23 tokens per second)\n",
      "llama_print_timings:        eval time =    3698.58 ms /   255 runs   (   14.50 ms per token,    68.95 tokens per second)\n",
      "llama_print_timings:       total time =    4057.69 ms /   490 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     115.75 ms /   197 runs   (    0.59 ms per token,  1701.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     297.33 ms /   914 tokens (    0.33 ms per token,  3074.04 tokens per second)\n",
      "llama_print_timings:        eval time =    2931.58 ms /   196 runs   (   14.96 ms per token,    66.86 tokens per second)\n",
      "llama_print_timings:       total time =    3441.49 ms /  1110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.47 ms /   256 runs   (    0.58 ms per token,  1712.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     220.91 ms /   609 tokens (    0.36 ms per token,  2756.75 tokens per second)\n",
      "llama_print_timings:        eval time =    3785.32 ms /   255 runs   (   14.84 ms per token,    67.37 tokens per second)\n",
      "llama_print_timings:       total time =    4290.18 ms /   864 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      84.51 ms /   144 runs   (    0.59 ms per token,  1703.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.94 ms /   354 tokens (    0.30 ms per token,  3310.30 tokens per second)\n",
      "llama_print_timings:        eval time =    2069.64 ms /   143 runs   (   14.47 ms per token,    69.09 tokens per second)\n",
      "llama_print_timings:       total time =    2325.14 ms /   497 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.19 ms /   256 runs   (    0.59 ms per token,  1693.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.23 ms /   439 tokens (    0.31 ms per token,  3246.35 tokens per second)\n",
      "llama_print_timings:        eval time =    3753.34 ms /   255 runs   (   14.72 ms per token,    67.94 tokens per second)\n",
      "llama_print_timings:       total time =    4170.26 ms /   694 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      60.17 ms /   104 runs   (    0.58 ms per token,  1728.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.57 ms /   350 tokens (    0.30 ms per token,  3284.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1490.27 ms /   103 runs   (   14.47 ms per token,    69.11 tokens per second)\n",
      "llama_print_timings:       total time =    1701.33 ms /   453 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.33 ms /   256 runs   (    0.59 ms per token,  1691.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     220.50 ms /   605 tokens (    0.36 ms per token,  2743.74 tokens per second)\n",
      "llama_print_timings:        eval time =    3783.66 ms /   255 runs   (   14.84 ms per token,    67.40 tokens per second)\n",
      "llama_print_timings:       total time =    4286.39 ms /   860 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     128.17 ms /   216 runs   (    0.59 ms per token,  1685.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.71 ms /   396 tokens (    0.33 ms per token,  3029.58 tokens per second)\n",
      "llama_print_timings:        eval time =    3144.13 ms /   215 runs   (   14.62 ms per token,    68.38 tokens per second)\n",
      "llama_print_timings:       total time =    3507.20 ms /   611 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      78.66 ms /   132 runs   (    0.60 ms per token,  1678.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     128.99 ms /   387 tokens (    0.33 ms per token,  3000.21 tokens per second)\n",
      "llama_print_timings:        eval time =    1903.21 ms /   131 runs   (   14.53 ms per token,    68.83 tokens per second)\n",
      "llama_print_timings:       total time =    2169.45 ms /   518 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      56.44 ms /    98 runs   (    0.58 ms per token,  1736.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.96 ms /   469 tokens (    0.30 ms per token,  3351.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1425.64 ms /    97 runs   (   14.70 ms per token,    68.04 tokens per second)\n",
      "llama_print_timings:       total time =    1662.84 ms /   566 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.37 ms /   256 runs   (    0.59 ms per token,  1691.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     366.95 ms /  1083 tokens (    0.34 ms per token,  2951.34 tokens per second)\n",
      "llama_print_timings:        eval time =    3821.53 ms /   255 runs   (   14.99 ms per token,    66.73 tokens per second)\n",
      "llama_print_timings:       total time =    4477.14 ms /  1338 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      48.01 ms /    85 runs   (    0.56 ms per token,  1770.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     112.36 ms /   384 tokens (    0.29 ms per token,  3417.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1217.47 ms /    84 runs   (   14.49 ms per token,    69.00 tokens per second)\n",
      "llama_print_timings:       total time =    1414.43 ms /   468 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     154.21 ms /   256 runs   (    0.60 ms per token,  1660.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     182.26 ms /   524 tokens (    0.35 ms per token,  2875.08 tokens per second)\n",
      "llama_print_timings:        eval time =    3778.71 ms /   255 runs   (   14.82 ms per token,    67.48 tokens per second)\n",
      "llama_print_timings:       total time =    4248.65 ms /   779 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     127.63 ms /   212 runs   (    0.60 ms per token,  1661.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     169.28 ms /   517 tokens (    0.33 ms per token,  3054.04 tokens per second)\n",
      "llama_print_timings:        eval time =    3124.55 ms /   211 runs   (   14.81 ms per token,    67.53 tokens per second)\n",
      "llama_print_timings:       total time =    3525.28 ms /   728 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      74.58 ms /   123 runs   (    0.61 ms per token,  1649.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     248.80 ms /   737 tokens (    0.34 ms per token,  2962.17 tokens per second)\n",
      "llama_print_timings:        eval time =    1832.92 ms /   122 runs   (   15.02 ms per token,    66.56 tokens per second)\n",
      "llama_print_timings:       total time =    2214.49 ms /   859 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      91.96 ms /   154 runs   (    0.60 ms per token,  1674.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     301.39 ms /   938 tokens (    0.32 ms per token,  3112.27 tokens per second)\n",
      "llama_print_timings:        eval time =    2290.17 ms /   153 runs   (   14.97 ms per token,    66.81 tokens per second)\n",
      "llama_print_timings:       total time =    2756.53 ms /  1091 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      60.77 ms /   102 runs   (    0.60 ms per token,  1678.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.49 ms /   358 tokens (    0.30 ms per token,  3330.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1462.91 ms /   101 runs   (   14.48 ms per token,    69.04 tokens per second)\n",
      "llama_print_timings:       total time =    1674.13 ms /   459 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      62.12 ms /   108 runs   (    0.58 ms per token,  1738.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     201.96 ms /   573 tokens (    0.35 ms per token,  2837.17 tokens per second)\n",
      "llama_print_timings:        eval time =    1583.33 ms /   107 runs   (   14.80 ms per token,    67.58 tokens per second)\n",
      "llama_print_timings:       total time =    1893.01 ms /   680 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     102.58 ms /   173 runs   (    0.59 ms per token,  1686.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     200.54 ms /   564 tokens (    0.36 ms per token,  2812.39 tokens per second)\n",
      "llama_print_timings:        eval time =    2546.58 ms /   172 runs   (   14.81 ms per token,    67.54 tokens per second)\n",
      "llama_print_timings:       total time =    2930.86 ms /   736 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     118.33 ms /   202 runs   (    0.59 ms per token,  1707.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.28 ms /   341 tokens (    0.31 ms per token,  3239.07 tokens per second)\n",
      "llama_print_timings:        eval time =    2917.56 ms /   201 runs   (   14.52 ms per token,    68.89 tokens per second)\n",
      "llama_print_timings:       total time =    3236.80 ms /   542 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      47.66 ms /    80 runs   (    0.60 ms per token,  1678.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =      81.21 ms /   213 tokens (    0.38 ms per token,  2622.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1136.40 ms /    79 runs   (   14.38 ms per token,    69.52 tokens per second)\n",
      "llama_print_timings:       total time =    1298.67 ms /   292 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      78.36 ms /   135 runs   (    0.58 ms per token,  1722.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.58 ms /   297 tokens (    0.34 ms per token,  2923.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1930.58 ms /   134 runs   (   14.41 ms per token,    69.41 tokens per second)\n",
      "llama_print_timings:       total time =    2168.96 ms /   431 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      81.39 ms /   136 runs   (    0.60 ms per token,  1670.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.54 ms /   345 tokens (    0.31 ms per token,  3238.22 tokens per second)\n",
      "llama_print_timings:        eval time =    1952.87 ms /   135 runs   (   14.47 ms per token,    69.13 tokens per second)\n",
      "llama_print_timings:       total time =    2202.10 ms /   480 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      72.05 ms /   120 runs   (    0.60 ms per token,  1665.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.84 ms /   292 tokens (    0.35 ms per token,  2867.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1726.01 ms /   119 runs   (   14.50 ms per token,    68.94 tokens per second)\n",
      "llama_print_timings:       total time =    1954.08 ms /   411 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     152.01 ms /   256 runs   (    0.59 ms per token,  1684.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     220.15 ms /   601 tokens (    0.37 ms per token,  2729.94 tokens per second)\n",
      "llama_print_timings:        eval time =    3793.85 ms /   255 runs   (   14.88 ms per token,    67.21 tokens per second)\n",
      "llama_print_timings:       total time =    4301.77 ms /   856 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      35.56 ms /    60 runs   (    0.59 ms per token,  1687.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.78 ms /   593 tokens (    0.37 ms per token,  2698.14 tokens per second)\n",
      "llama_print_timings:        eval time =     881.76 ms /    59 runs   (   14.95 ms per token,    66.91 tokens per second)\n",
      "llama_print_timings:       total time =    1164.90 ms /   652 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      52.81 ms /    88 runs   (    0.60 ms per token,  1666.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.30 ms /   276 tokens (    0.36 ms per token,  2751.74 tokens per second)\n",
      "llama_print_timings:        eval time =    1259.20 ms /    87 runs   (   14.47 ms per token,    69.09 tokens per second)\n",
      "llama_print_timings:       total time =    1450.72 ms /   363 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      87.66 ms /   149 runs   (    0.59 ms per token,  1699.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     189.32 ms /   548 tokens (    0.35 ms per token,  2894.51 tokens per second)\n",
      "llama_print_timings:        eval time =    2196.37 ms /   148 runs   (   14.84 ms per token,    67.38 tokens per second)\n",
      "llama_print_timings:       total time =    2542.12 ms /   696 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      91.96 ms /   153 runs   (    0.60 ms per token,  1663.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     142.88 ms /   417 tokens (    0.34 ms per token,  2918.51 tokens per second)\n",
      "llama_print_timings:        eval time =    2269.53 ms /   152 runs   (   14.93 ms per token,    66.97 tokens per second)\n",
      "llama_print_timings:       total time =    2580.34 ms /   569 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      52.18 ms /    87 runs   (    0.60 ms per token,  1667.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.99 ms /   416 tokens (    0.32 ms per token,  3128.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1256.11 ms /    86 runs   (   14.61 ms per token,    68.47 tokens per second)\n",
      "llama_print_timings:       total time =    1479.02 ms /   502 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.43 ms /   256 runs   (    0.59 ms per token,  1690.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.41 ms /   592 tokens (    0.37 ms per token,  2698.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3788.22 ms /   255 runs   (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_print_timings:       total time =    4292.40 ms /   847 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      40.41 ms /    71 runs   (    0.57 ms per token,  1756.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.48 ms /   411 tokens (    0.32 ms per token,  3102.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1015.03 ms /    70 runs   (   14.50 ms per token,    68.96 tokens per second)\n",
      "llama_print_timings:       total time =    1216.70 ms /   481 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      76.43 ms /   129 runs   (    0.59 ms per token,  1687.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.42 ms /   602 tokens (    0.36 ms per token,  2743.55 tokens per second)\n",
      "llama_print_timings:        eval time =    1898.39 ms /   128 runs   (   14.83 ms per token,    67.43 tokens per second)\n",
      "llama_print_timings:       total time =    2251.99 ms /   730 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      25.26 ms /    43 runs   (    0.59 ms per token,  1702.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.14 ms /   330 tokens (    0.32 ms per token,  3138.79 tokens per second)\n",
      "llama_print_timings:        eval time =     606.23 ms /    42 runs   (   14.43 ms per token,    69.28 tokens per second)\n",
      "llama_print_timings:       total time =     754.17 ms /   372 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     111.41 ms /   193 runs   (    0.58 ms per token,  1732.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.08 ms /   400 tokens (    0.33 ms per token,  3051.64 tokens per second)\n",
      "llama_print_timings:        eval time =    2806.37 ms /   192 runs   (   14.62 ms per token,    68.42 tokens per second)\n",
      "llama_print_timings:       total time =    3138.43 ms /   592 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     153.06 ms /   256 runs   (    0.60 ms per token,  1672.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.74 ms /   301 tokens (    0.34 ms per token,  2929.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3723.86 ms /   255 runs   (   14.60 ms per token,    68.48 tokens per second)\n",
      "llama_print_timings:       total time =    4110.18 ms /   556 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.73 ms /   256 runs   (    0.58 ms per token,  1732.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.25 ms /   398 tokens (    0.33 ms per token,  3055.62 tokens per second)\n",
      "llama_print_timings:        eval time =    3735.83 ms /   255 runs   (   14.65 ms per token,    68.26 tokens per second)\n",
      "llama_print_timings:       total time =    4143.27 ms /   653 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     144.96 ms /   256 runs   (    0.57 ms per token,  1766.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     347.15 ms /  1040 tokens (    0.33 ms per token,  2995.85 tokens per second)\n",
      "llama_print_timings:        eval time =    3816.19 ms /   255 runs   (   14.97 ms per token,    66.82 tokens per second)\n",
      "llama_print_timings:       total time =    4439.58 ms /  1295 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      70.83 ms /   124 runs   (    0.57 ms per token,  1750.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     132.55 ms /   421 tokens (    0.31 ms per token,  3176.26 tokens per second)\n",
      "llama_print_timings:        eval time =    1794.54 ms /   123 runs   (   14.59 ms per token,    68.54 tokens per second)\n",
      "llama_print_timings:       total time =    2051.23 ms /   544 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      96.37 ms /   163 runs   (    0.59 ms per token,  1691.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     130.52 ms /   393 tokens (    0.33 ms per token,  3011.15 tokens per second)\n",
      "llama_print_timings:        eval time =    2365.55 ms /   162 runs   (   14.60 ms per token,    68.48 tokens per second)\n",
      "llama_print_timings:       total time =    2666.59 ms /   555 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     144.52 ms /   256 runs   (    0.56 ms per token,  1771.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     434.68 ms /  1320 tokens (    0.33 ms per token,  3036.74 tokens per second)\n",
      "llama_print_timings:        eval time =    3861.82 ms /   255 runs   (   15.14 ms per token,    66.03 tokens per second)\n",
      "llama_print_timings:       total time =    4576.80 ms /  1575 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     105.48 ms /   186 runs   (    0.57 ms per token,  1763.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.39 ms /   402 tokens (    0.33 ms per token,  3059.57 tokens per second)\n",
      "llama_print_timings:        eval time =    2702.93 ms /   185 runs   (   14.61 ms per token,    68.44 tokens per second)\n",
      "llama_print_timings:       total time =    3025.24 ms /   587 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     125.99 ms /   216 runs   (    0.58 ms per token,  1714.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.98 ms /   300 tokens (    0.36 ms per token,  2752.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3181.59 ms /   215 runs   (   14.80 ms per token,    67.58 tokens per second)\n",
      "llama_print_timings:       total time =    3520.81 ms /   515 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      70.08 ms /   121 runs   (    0.58 ms per token,  1726.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.76 ms /   288 tokens (    0.35 ms per token,  2858.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1729.24 ms /   120 runs   (   14.41 ms per token,    69.39 tokens per second)\n",
      "llama_print_timings:       total time =    1951.66 ms /   408 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.27 ms /   256 runs   (    0.58 ms per token,  1715.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =      78.80 ms /   202 tokens (    0.39 ms per token,  2563.39 tokens per second)\n",
      "llama_print_timings:        eval time =    3659.90 ms /   255 runs   (   14.35 ms per token,    69.67 tokens per second)\n",
      "llama_print_timings:       total time =    4017.26 ms /   457 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     146.43 ms /   256 runs   (    0.57 ms per token,  1748.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     270.59 ms /   858 tokens (    0.32 ms per token,  3170.80 tokens per second)\n",
      "llama_print_timings:        eval time =    3808.67 ms /   255 runs   (   14.94 ms per token,    66.95 tokens per second)\n",
      "llama_print_timings:       total time =    4358.12 ms /  1113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     144.83 ms /   256 runs   (    0.57 ms per token,  1767.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.46 ms /   428 tokens (    0.31 ms per token,  3183.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3746.85 ms /   255 runs   (   14.69 ms per token,    68.06 tokens per second)\n",
      "llama_print_timings:       total time =    4155.86 ms /   683 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      27.18 ms /    47 runs   (    0.58 ms per token,  1729.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     131.21 ms /   405 tokens (    0.32 ms per token,  3086.65 tokens per second)\n",
      "llama_print_timings:        eval time =     665.57 ms /    46 runs   (   14.47 ms per token,    69.11 tokens per second)\n",
      "llama_print_timings:       total time =     842.63 ms /   451 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      80.39 ms /   142 runs   (    0.57 ms per token,  1766.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.55 ms /   451 tokens (    0.30 ms per token,  3278.88 tokens per second)\n",
      "llama_print_timings:        eval time =    2071.04 ms /   141 runs   (   14.69 ms per token,    68.08 tokens per second)\n",
      "llama_print_timings:       total time =    2351.23 ms /   592 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      72.17 ms /   122 runs   (    0.59 ms per token,  1690.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =      82.55 ms /   225 tokens (    0.37 ms per token,  2725.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1744.19 ms /   121 runs   (   14.41 ms per token,    69.37 tokens per second)\n",
      "llama_print_timings:       total time =    1951.53 ms /   346 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     153.53 ms /   256 runs   (    0.60 ms per token,  1667.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     467.64 ms /  1414 tokens (    0.33 ms per token,  3023.66 tokens per second)\n",
      "llama_print_timings:        eval time =    3863.88 ms /   255 runs   (   15.15 ms per token,    66.00 tokens per second)\n",
      "llama_print_timings:       total time =    4625.06 ms /  1669 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.40 ms /   256 runs   (    0.58 ms per token,  1736.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     110.35 ms /   377 tokens (    0.29 ms per token,  3416.28 tokens per second)\n",
      "llama_print_timings:        eval time =    3727.13 ms /   255 runs   (   14.62 ms per token,    68.42 tokens per second)\n",
      "llama_print_timings:       total time =    4112.27 ms /   632 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.78 ms /   256 runs   (    0.58 ms per token,  1732.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     108.55 ms /   362 tokens (    0.30 ms per token,  3334.84 tokens per second)\n",
      "llama_print_timings:        eval time =    3718.15 ms /   255 runs   (   14.58 ms per token,    68.58 tokens per second)\n",
      "llama_print_timings:       total time =    4103.10 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      60.89 ms /   104 runs   (    0.59 ms per token,  1707.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.69 ms /   279 tokens (    0.36 ms per token,  2770.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1505.48 ms /   103 runs   (   14.62 ms per token,    68.42 tokens per second)\n",
      "llama_print_timings:       total time =    1711.44 ms /   382 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     152.97 ms /   255 runs   (    0.60 ms per token,  1666.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.85 ms /   426 tokens (    0.31 ms per token,  3182.55 tokens per second)\n",
      "llama_print_timings:        eval time =    3734.57 ms /   254 runs   (   14.70 ms per token,    68.01 tokens per second)\n",
      "llama_print_timings:       total time =    4151.55 ms /   680 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     117.81 ms /   200 runs   (    0.59 ms per token,  1697.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =      79.11 ms /   173 tokens (    0.46 ms per token,  2186.69 tokens per second)\n",
      "llama_print_timings:        eval time =    2914.46 ms /   199 runs   (   14.65 ms per token,    68.28 tokens per second)\n",
      "llama_print_timings:       total time =    3206.49 ms /   372 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      51.15 ms /    85 runs   (    0.60 ms per token,  1661.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.71 ms /   435 tokens (    0.31 ms per token,  3229.06 tokens per second)\n",
      "llama_print_timings:        eval time =    1223.92 ms /    84 runs   (   14.57 ms per token,    68.63 tokens per second)\n",
      "llama_print_timings:       total time =    1444.24 ms /   519 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      74.30 ms /   130 runs   (    0.57 ms per token,  1749.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.32 ms /   428 tokens (    0.31 ms per token,  3186.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1885.38 ms /   129 runs   (   14.62 ms per token,    68.42 tokens per second)\n",
      "llama_print_timings:       total time =    2149.84 ms /   557 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      75.13 ms /   129 runs   (    0.58 ms per token,  1716.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.08 ms /   336 tokens (    0.31 ms per token,  3197.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1848.42 ms /   128 runs   (   14.44 ms per token,    69.25 tokens per second)\n",
      "llama_print_timings:       total time =    2084.25 ms /   464 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      70.88 ms /   123 runs   (    0.58 ms per token,  1735.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =      80.03 ms /   194 tokens (    0.41 ms per token,  2424.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1761.83 ms /   122 runs   (   14.44 ms per token,    69.25 tokens per second)\n",
      "llama_print_timings:       total time =    1965.26 ms /   316 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     126.41 ms /   209 runs   (    0.60 ms per token,  1653.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.37 ms /   434 tokens (    0.31 ms per token,  3206.00 tokens per second)\n",
      "llama_print_timings:        eval time =    3058.35 ms /   208 runs   (   14.70 ms per token,    68.01 tokens per second)\n",
      "llama_print_timings:       total time =    3424.16 ms /   642 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      92.84 ms /   156 runs   (    0.60 ms per token,  1680.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =      30.73 ms /    25 tokens (    1.23 ms per token,   813.59 tokens per second)\n",
      "llama_print_timings:        eval time =    2272.49 ms /   155 runs   (   14.66 ms per token,    68.21 tokens per second)\n",
      "llama_print_timings:       total time =    2467.66 ms /   180 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      58.88 ms /   102 runs   (    0.58 ms per token,  1732.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.20 ms /   345 tokens (    0.31 ms per token,  3248.43 tokens per second)\n",
      "llama_print_timings:        eval time =    1458.84 ms /   101 runs   (   14.44 ms per token,    69.23 tokens per second)\n",
      "llama_print_timings:       total time =    1666.70 ms /   446 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      37.47 ms /    62 runs   (    0.60 ms per token,  1654.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     137.97 ms /   456 tokens (    0.30 ms per token,  3304.97 tokens per second)\n",
      "llama_print_timings:        eval time =     891.61 ms /    61 runs   (   14.62 ms per token,    68.42 tokens per second)\n",
      "llama_print_timings:       total time =    1093.21 ms /   517 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      87.63 ms /   150 runs   (    0.58 ms per token,  1711.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     134.28 ms /   430 tokens (    0.31 ms per token,  3202.22 tokens per second)\n",
      "llama_print_timings:        eval time =    2182.22 ms /   149 runs   (   14.65 ms per token,    68.28 tokens per second)\n",
      "llama_print_timings:       total time =    2471.17 ms /   579 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     150.59 ms /   256 runs   (    0.59 ms per token,  1699.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.03 ms /   337 tokens (    0.31 ms per token,  3208.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3714.96 ms /   255 runs   (   14.57 ms per token,    68.64 tokens per second)\n",
      "llama_print_timings:       total time =    4100.68 ms /   592 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      89.69 ms /   152 runs   (    0.59 ms per token,  1694.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.20 ms /   435 tokens (    0.31 ms per token,  3217.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2218.93 ms /   151 runs   (   14.69 ms per token,    68.05 tokens per second)\n",
      "llama_print_timings:       total time =    2513.04 ms /   586 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     148.59 ms /   256 runs   (    0.58 ms per token,  1722.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     242.20 ms /   728 tokens (    0.33 ms per token,  3005.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3798.41 ms /   255 runs   (   14.90 ms per token,    67.13 tokens per second)\n",
      "llama_print_timings:       total time =    4322.49 ms /   983 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      64.26 ms /   109 runs   (    0.59 ms per token,  1696.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     239.38 ms /   710 tokens (    0.34 ms per token,  2965.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1607.33 ms /   108 runs   (   14.88 ms per token,    67.19 tokens per second)\n",
      "llama_print_timings:       total time =    1958.67 ms /   818 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     146.24 ms /   256 runs   (    0.57 ms per token,  1750.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.84 ms /   448 tokens (    0.30 ms per token,  3298.09 tokens per second)\n",
      "llama_print_timings:        eval time =    3754.92 ms /   255 runs   (   14.73 ms per token,    67.91 tokens per second)\n",
      "llama_print_timings:       total time =    4167.52 ms /   703 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      47.66 ms /    84 runs   (    0.57 ms per token,  1762.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     233.99 ms /   668 tokens (    0.35 ms per token,  2854.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1233.30 ms /    83 runs   (   14.86 ms per token,    67.30 tokens per second)\n",
      "llama_print_timings:       total time =    1550.06 ms /   751 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     134.10 ms /   229 runs   (    0.59 ms per token,  1707.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     242.08 ms /   733 tokens (    0.33 ms per token,  3027.91 tokens per second)\n",
      "llama_print_timings:        eval time =    3396.14 ms /   228 runs   (   14.90 ms per token,    67.14 tokens per second)\n",
      "llama_print_timings:       total time =    3888.11 ms /   961 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      36.81 ms /    67 runs   (    0.55 ms per token,  1820.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     246.19 ms /   759 tokens (    0.32 ms per token,  3083.00 tokens per second)\n",
      "llama_print_timings:        eval time =     983.02 ms /    66 runs   (   14.89 ms per token,    67.14 tokens per second)\n",
      "llama_print_timings:       total time =    1294.01 ms /   825 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     145.58 ms /   256 runs   (    0.57 ms per token,  1758.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.81 ms /   530 tokens (    0.34 ms per token,  2915.21 tokens per second)\n",
      "llama_print_timings:        eval time =    3778.84 ms /   255 runs   (   14.82 ms per token,    67.48 tokens per second)\n",
      "llama_print_timings:       total time =    4237.11 ms /   785 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     106.86 ms /   188 runs   (    0.57 ms per token,  1759.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     470.39 ms /  1433 tokens (    0.33 ms per token,  3046.43 tokens per second)\n",
      "llama_print_timings:        eval time =    2833.77 ms /   187 runs   (   15.15 ms per token,    65.99 tokens per second)\n",
      "llama_print_timings:       total time =    3504.23 ms /  1620 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      37.10 ms /    66 runs   (    0.56 ms per token,  1778.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     224.01 ms /   568 tokens (    0.39 ms per token,  2535.61 tokens per second)\n",
      "llama_print_timings:        eval time =     980.29 ms /    65 runs   (   15.08 ms per token,    66.31 tokens per second)\n",
      "llama_print_timings:       total time =    1270.52 ms /   633 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      40.98 ms /    71 runs   (    0.58 ms per token,  1732.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     272.88 ms /   871 tokens (    0.31 ms per token,  3191.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1047.41 ms /    70 runs   (   14.96 ms per token,    66.83 tokens per second)\n",
      "llama_print_timings:       total time =    1391.79 ms /   941 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      67.03 ms /   114 runs   (    0.59 ms per token,  1700.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     521.74 ms /  1555 tokens (    0.34 ms per token,  2980.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1699.18 ms /   113 runs   (   15.04 ms per token,    66.50 tokens per second)\n",
      "llama_print_timings:       total time =    2342.64 ms /  1668 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     148.54 ms /   256 runs   (    0.58 ms per token,  1723.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     315.72 ms /  1020 tokens (    0.31 ms per token,  3230.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3829.62 ms /   255 runs   (   15.02 ms per token,    66.59 tokens per second)\n",
      "llama_print_timings:       total time =    4431.22 ms /  1275 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     146.90 ms /   256 runs   (    0.57 ms per token,  1742.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     256.61 ms /   670 tokens (    0.38 ms per token,  2610.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3848.27 ms /   255 runs   (   15.09 ms per token,    66.26 tokens per second)\n",
      "llama_print_timings:       total time =    4392.85 ms /   925 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      94.03 ms /   163 runs   (    0.58 ms per token,  1733.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     432.00 ms /  1301 tokens (    0.33 ms per token,  3011.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2449.74 ms /   162 runs   (   15.12 ms per token,    66.13 tokens per second)\n",
      "llama_print_timings:       total time =    3053.38 ms /  1463 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     150.26 ms /   256 runs   (    0.59 ms per token,  1703.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     508.65 ms /  1543 tokens (    0.33 ms per token,  3033.55 tokens per second)\n",
      "llama_print_timings:        eval time =    3841.42 ms /   255 runs   (   15.06 ms per token,    66.38 tokens per second)\n",
      "llama_print_timings:       total time =    4639.40 ms /  1798 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      71.02 ms /   120 runs   (    0.59 ms per token,  1689.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     298.35 ms /   920 tokens (    0.32 ms per token,  3083.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1780.00 ms /   119 runs   (   14.96 ms per token,    66.85 tokens per second)\n",
      "llama_print_timings:       total time =    2203.24 ms /  1039 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     150.88 ms /   256 runs   (    0.59 ms per token,  1696.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     243.43 ms /   744 tokens (    0.33 ms per token,  3056.28 tokens per second)\n",
      "llama_print_timings:        eval time =    3797.78 ms /   255 runs   (   14.89 ms per token,    67.14 tokens per second)\n",
      "llama_print_timings:       total time =    4323.66 ms /   999 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     148.80 ms /   256 runs   (    0.58 ms per token,  1720.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.01 ms /   884 tokens (    0.31 ms per token,  3214.38 tokens per second)\n",
      "llama_print_timings:        eval time =    3812.03 ms /   255 runs   (   14.95 ms per token,    66.89 tokens per second)\n",
      "llama_print_timings:       total time =    4367.85 ms /  1139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     143.51 ms /   256 runs   (    0.56 ms per token,  1783.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     270.81 ms /   857 tokens (    0.32 ms per token,  3164.56 tokens per second)\n",
      "llama_print_timings:        eval time =    3811.24 ms /   255 runs   (   14.95 ms per token,    66.91 tokens per second)\n",
      "llama_print_timings:       total time =    4357.87 ms /  1112 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.66 ms /   256 runs   (    0.58 ms per token,  1710.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     521.72 ms /  1548 tokens (    0.34 ms per token,  2967.11 tokens per second)\n",
      "llama_print_timings:        eval time =    3838.12 ms /   255 runs   (   15.05 ms per token,    66.44 tokens per second)\n",
      "llama_print_timings:       total time =    4646.62 ms /  1803 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      46.12 ms /    79 runs   (    0.58 ms per token,  1713.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     405.98 ms /  1211 tokens (    0.34 ms per token,  2982.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1168.92 ms /    78 runs   (   14.99 ms per token,    66.73 tokens per second)\n",
      "llama_print_timings:       total time =    1656.65 ms /  1289 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      60.90 ms /   102 runs   (    0.60 ms per token,  1674.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     316.31 ms /  1024 tokens (    0.31 ms per token,  3237.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1531.39 ms /   102 runs   (   15.01 ms per token,    66.61 tokens per second)\n",
      "llama_print_timings:       total time =    1954.84 ms /  1126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      52.24 ms /    90 runs   (    0.58 ms per token,  1722.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.01 ms /   879 tokens (    0.31 ms per token,  3207.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1328.00 ms /    89 runs   (   14.92 ms per token,    67.02 tokens per second)\n",
      "llama_print_timings:       total time =    1692.67 ms /   968 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     117.27 ms /   208 runs   (    0.56 ms per token,  1773.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     403.79 ms /  1193 tokens (    0.34 ms per token,  2954.54 tokens per second)\n",
      "llama_print_timings:        eval time =    3097.93 ms /   207 runs   (   14.97 ms per token,    66.82 tokens per second)\n",
      "llama_print_timings:       total time =    3724.06 ms /  1400 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      92.63 ms /   165 runs   (    0.56 ms per token,  1781.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     350.85 ms /  1054 tokens (    0.33 ms per token,  3004.18 tokens per second)\n",
      "llama_print_timings:        eval time =    2461.24 ms /   164 runs   (   15.01 ms per token,    66.63 tokens per second)\n",
      "llama_print_timings:       total time =    2983.08 ms /  1218 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      84.81 ms /   144 runs   (    0.59 ms per token,  1697.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     305.91 ms /   971 tokens (    0.32 ms per token,  3174.18 tokens per second)\n",
      "llama_print_timings:        eval time =    2142.59 ms /   143 runs   (   14.98 ms per token,    66.74 tokens per second)\n",
      "llama_print_timings:       total time =    2601.37 ms /  1114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     139.16 ms /   256 runs   (    0.54 ms per token,  1839.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     354.41 ms /  1062 tokens (    0.33 ms per token,  2996.50 tokens per second)\n",
      "llama_print_timings:        eval time =    3816.69 ms /   255 runs   (   14.97 ms per token,    66.81 tokens per second)\n",
      "llama_print_timings:       total time =    4443.47 ms /  1317 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.38 ms /   256 runs   (    0.58 ms per token,  1713.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     480.91 ms /  1485 tokens (    0.32 ms per token,  3087.87 tokens per second)\n",
      "llama_print_timings:        eval time =    3846.58 ms /   255 runs   (   15.08 ms per token,    66.29 tokens per second)\n",
      "llama_print_timings:       total time =    4616.65 ms /  1740 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     146.74 ms /   256 runs   (    0.57 ms per token,  1744.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =      33.12 ms /    26 tokens (    1.27 ms per token,   785.10 tokens per second)\n",
      "llama_print_timings:        eval time =    3850.57 ms /   255 runs   (   15.10 ms per token,    66.22 tokens per second)\n",
      "llama_print_timings:       total time =    4172.17 ms /   281 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     113.28 ms /   203 runs   (    0.56 ms per token,  1791.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     410.85 ms /  1245 tokens (    0.33 ms per token,  3030.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3041.89 ms /   202 runs   (   15.06 ms per token,    66.41 tokens per second)\n",
      "llama_print_timings:       total time =    3667.46 ms /  1447 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      75.48 ms /   127 runs   (    0.59 ms per token,  1682.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     272.45 ms /   870 tokens (    0.31 ms per token,  3193.25 tokens per second)\n",
      "llama_print_timings:        eval time =    1879.08 ms /   126 runs   (   14.91 ms per token,    67.05 tokens per second)\n",
      "llama_print_timings:       total time =    2284.20 ms /   996 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      56.13 ms /    96 runs   (    0.58 ms per token,  1710.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =      77.96 ms /   170 tokens (    0.46 ms per token,  2180.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1375.47 ms /    95 runs   (   14.48 ms per token,    69.07 tokens per second)\n",
      "llama_print_timings:       total time =    1550.35 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     143.18 ms /   243 runs   (    0.59 ms per token,  1697.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.66 ms /   811 tokens (    0.33 ms per token,  3052.77 tokens per second)\n",
      "llama_print_timings:        eval time =    3609.69 ms /   242 runs   (   14.92 ms per token,    67.04 tokens per second)\n",
      "llama_print_timings:       total time =    4141.75 ms /  1053 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     112.12 ms /   196 runs   (    0.57 ms per token,  1748.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     521.64 ms /  1546 tokens (    0.34 ms per token,  2963.73 tokens per second)\n",
      "llama_print_timings:        eval time =    2934.43 ms /   195 runs   (   15.05 ms per token,    66.45 tokens per second)\n",
      "llama_print_timings:       total time =    3668.04 ms /  1741 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     142.30 ms /   248 runs   (    0.57 ms per token,  1742.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     299.54 ms /   927 tokens (    0.32 ms per token,  3094.72 tokens per second)\n",
      "llama_print_timings:        eval time =    3694.87 ms /   247 runs   (   14.96 ms per token,    66.85 tokens per second)\n",
      "llama_print_timings:       total time =    4261.65 ms /  1174 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     128.26 ms /   218 runs   (    0.59 ms per token,  1699.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     391.71 ms /  1146 tokens (    0.34 ms per token,  2925.64 tokens per second)\n",
      "llama_print_timings:        eval time =    3242.94 ms /   217 runs   (   14.94 ms per token,    66.91 tokens per second)\n",
      "llama_print_timings:       total time =    3872.27 ms /  1363 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      72.58 ms /   122 runs   (    0.59 ms per token,  1680.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.61 ms /   337 tokens (    0.31 ms per token,  3190.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1753.70 ms /   121 runs   (   14.49 ms per token,    69.00 tokens per second)\n",
      "llama_print_timings:       total time =    1987.48 ms /   458 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      93.11 ms /   155 runs   (    0.60 ms per token,  1664.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     223.22 ms /   622 tokens (    0.36 ms per token,  2786.48 tokens per second)\n",
      "llama_print_timings:        eval time =    2287.98 ms /   154 runs   (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_print_timings:       total time =    2677.42 ms /   776 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      36.49 ms /    65 runs   (    0.56 ms per token,  1781.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.66 ms /   349 tokens (    0.31 ms per token,  3271.96 tokens per second)\n",
      "llama_print_timings:        eval time =     925.45 ms /    64 runs   (   14.46 ms per token,    69.16 tokens per second)\n",
      "llama_print_timings:       total time =    1094.68 ms /   413 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.95 ms /   256 runs   (    0.58 ms per token,  1730.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.03 ms /   802 tokens (    0.33 ms per token,  3037.55 tokens per second)\n",
      "llama_print_timings:        eval time =    3806.01 ms /   255 runs   (   14.93 ms per token,    67.00 tokens per second)\n",
      "llama_print_timings:       total time =    4351.25 ms /  1057 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.20 ms /   256 runs   (    0.58 ms per token,  1715.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     245.22 ms /   752 tokens (    0.33 ms per token,  3066.65 tokens per second)\n",
      "llama_print_timings:        eval time =    3797.78 ms /   255 runs   (   14.89 ms per token,    67.14 tokens per second)\n",
      "llama_print_timings:       total time =    4323.06 ms /  1007 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      80.83 ms /   138 runs   (    0.59 ms per token,  1707.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.89 ms /   517 tokens (    0.33 ms per token,  3061.16 tokens per second)\n",
      "llama_print_timings:        eval time =    2025.84 ms /   137 runs   (   14.79 ms per token,    67.63 tokens per second)\n",
      "llama_print_timings:       total time =    2335.93 ms /   654 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     102.92 ms /   172 runs   (    0.60 ms per token,  1671.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.32 ms /   460 tokens (    0.30 ms per token,  3325.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2519.09 ms /   171 runs   (   14.73 ms per token,    67.88 tokens per second)\n",
      "llama_print_timings:       total time =    2839.92 ms /   631 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      36.84 ms /    63 runs   (    0.58 ms per token,  1710.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.48 ms /   340 tokens (    0.31 ms per token,  3223.21 tokens per second)\n",
      "llama_print_timings:        eval time =     898.33 ms /    62 runs   (   14.49 ms per token,    69.02 tokens per second)\n",
      "llama_print_timings:       total time =    1066.48 ms /   402 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.31 ms /   250 runs   (    0.59 ms per token,  1697.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     106.42 ms /   345 tokens (    0.31 ms per token,  3241.75 tokens per second)\n",
      "llama_print_timings:        eval time =    3630.62 ms /   249 runs   (   14.58 ms per token,    68.58 tokens per second)\n",
      "llama_print_timings:       total time =    4009.69 ms /   594 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     145.92 ms /   256 runs   (    0.57 ms per token,  1754.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.06 ms /   886 tokens (    0.31 ms per token,  3221.06 tokens per second)\n",
      "llama_print_timings:        eval time =    3816.90 ms /   255 runs   (   14.97 ms per token,    66.81 tokens per second)\n",
      "llama_print_timings:       total time =    4371.76 ms /  1141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      93.29 ms /   156 runs   (    0.60 ms per token,  1672.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     102.26 ms /   304 tokens (    0.34 ms per token,  2972.90 tokens per second)\n",
      "llama_print_timings:        eval time =    2240.97 ms /   155 runs   (   14.46 ms per token,    69.17 tokens per second)\n",
      "llama_print_timings:       total time =    2508.40 ms /   459 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     150.71 ms /   256 runs   (    0.59 ms per token,  1698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     129.73 ms /   388 tokens (    0.33 ms per token,  2990.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3733.30 ms /   255 runs   (   14.64 ms per token,    68.30 tokens per second)\n",
      "llama_print_timings:       total time =    4143.41 ms /   643 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     147.24 ms /   256 runs   (    0.58 ms per token,  1738.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     439.41 ms /  1349 tokens (    0.33 ms per token,  3070.06 tokens per second)\n",
      "llama_print_timings:        eval time =    3867.76 ms /   255 runs   (   15.17 ms per token,    65.93 tokens per second)\n",
      "llama_print_timings:       total time =    4590.81 ms /  1604 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     111.28 ms /   187 runs   (    0.60 ms per token,  1680.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     129.24 ms /   390 tokens (    0.33 ms per token,  3017.76 tokens per second)\n",
      "llama_print_timings:        eval time =    2718.83 ms /   186 runs   (   14.62 ms per token,    68.41 tokens per second)\n",
      "llama_print_timings:       total time =    3048.70 ms /   576 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      43.30 ms /    73 runs   (    0.59 ms per token,  1686.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.10 ms /   365 tokens (    0.30 ms per token,  3345.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1044.77 ms /    72 runs   (   14.51 ms per token,    68.91 tokens per second)\n",
      "llama_print_timings:       total time =    1229.53 ms /   437 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      74.61 ms /   129 runs   (    0.58 ms per token,  1728.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.26 ms /   485 tokens (    0.29 ms per token,  3433.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1890.64 ms /   128 runs   (   14.77 ms per token,    67.70 tokens per second)\n",
      "llama_print_timings:       total time =    2163.61 ms /   613 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      25.30 ms /    43 runs   (    0.59 ms per token,  1699.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =      99.26 ms /   275 tokens (    0.36 ms per token,  2770.53 tokens per second)\n",
      "llama_print_timings:        eval time =     605.41 ms /    42 runs   (   14.41 ms per token,    69.37 tokens per second)\n",
      "llama_print_timings:       total time =     746.88 ms /   317 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      46.93 ms /    81 runs   (    0.58 ms per token,  1725.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.75 ms /   342 tokens (    0.31 ms per token,  3233.95 tokens per second)\n",
      "llama_print_timings:        eval time =    1154.74 ms /    80 runs   (   14.43 ms per token,    69.28 tokens per second)\n",
      "llama_print_timings:       total time =    1340.45 ms /   422 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.97 ms /   256 runs   (    0.59 ms per token,  1684.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =      26.90 ms /    22 tokens (    1.22 ms per token,   817.78 tokens per second)\n",
      "llama_print_timings:        eval time =    3722.13 ms /   255 runs   (   14.60 ms per token,    68.51 tokens per second)\n",
      "llama_print_timings:       total time =    4033.02 ms /   277 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     128.23 ms /   222 runs   (    0.58 ms per token,  1731.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     141.00 ms /   487 tokens (    0.29 ms per token,  3454.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3267.31 ms /   221 runs   (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_print_timings:       total time =    3645.05 ms /   708 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      58.42 ms /   104 runs   (    0.56 ms per token,  1780.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     224.18 ms /   629 tokens (    0.36 ms per token,  2805.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1530.29 ms /   103 runs   (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_print_timings:       total time =    1858.35 ms /   732 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.80 ms /   256 runs   (    0.59 ms per token,  1686.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     239.56 ms /   708 tokens (    0.34 ms per token,  2955.46 tokens per second)\n",
      "llama_print_timings:        eval time =    3802.12 ms /   255 runs   (   14.91 ms per token,    67.07 tokens per second)\n",
      "llama_print_timings:       total time =    4327.79 ms /   963 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      37.05 ms /    63 runs   (    0.59 ms per token,  1700.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     103.44 ms /   314 tokens (    0.33 ms per token,  3035.63 tokens per second)\n",
      "llama_print_timings:        eval time =     897.05 ms /    62 runs   (   14.47 ms per token,    69.12 tokens per second)\n",
      "llama_print_timings:       total time =    1064.00 ms /   376 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      48.72 ms /    80 runs   (    0.61 ms per token,  1642.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     103.08 ms /   320 tokens (    0.32 ms per token,  3104.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1143.08 ms /    79 runs   (   14.47 ms per token,    69.11 tokens per second)\n",
      "llama_print_timings:       total time =    1328.76 ms /   399 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      64.64 ms /   115 runs   (    0.56 ms per token,  1779.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.32 ms /   424 tokens (    0.31 ms per token,  3180.20 tokens per second)\n",
      "llama_print_timings:        eval time =    1665.46 ms /   114 runs   (   14.61 ms per token,    68.45 tokens per second)\n",
      "llama_print_timings:       total time =    1912.42 ms /   538 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      87.08 ms /   156 runs   (    0.56 ms per token,  1791.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.41 ms /   295 tokens (    0.34 ms per token,  2909.04 tokens per second)\n",
      "llama_print_timings:        eval time =    2236.08 ms /   155 runs   (   14.43 ms per token,    69.32 tokens per second)\n",
      "llama_print_timings:       total time =    2495.03 ms /   450 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     123.04 ms /   213 runs   (    0.58 ms per token,  1731.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     277.68 ms /   895 tokens (    0.31 ms per token,  3223.13 tokens per second)\n",
      "llama_print_timings:        eval time =    3174.92 ms /   212 runs   (   14.98 ms per token,    66.77 tokens per second)\n",
      "llama_print_timings:       total time =    3683.71 ms /  1107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      76.09 ms /   130 runs   (    0.59 ms per token,  1708.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     271.03 ms /   843 tokens (    0.32 ms per token,  3110.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1932.46 ms /   129 runs   (   14.98 ms per token,    66.75 tokens per second)\n",
      "llama_print_timings:       total time =    2339.82 ms /   972 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     144.61 ms /   252 runs   (    0.57 ms per token,  1742.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =      82.21 ms /   165 tokens (    0.50 ms per token,  2007.10 tokens per second)\n",
      "llama_print_timings:        eval time =    3736.99 ms /   251 runs   (   14.89 ms per token,    67.17 tokens per second)\n",
      "llama_print_timings:       total time =    4091.91 ms /   416 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     107.24 ms /   182 runs   (    0.59 ms per token,  1697.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     107.19 ms /   357 tokens (    0.30 ms per token,  3330.63 tokens per second)\n",
      "llama_print_timings:        eval time =    2635.15 ms /   181 runs   (   14.56 ms per token,    68.69 tokens per second)\n",
      "llama_print_timings:       total time =    2936.59 ms /   538 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      80.79 ms /   143 runs   (    0.56 ms per token,  1769.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     260.93 ms /   782 tokens (    0.33 ms per token,  2997.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2114.99 ms /   142 runs   (   14.89 ms per token,    67.14 tokens per second)\n",
      "llama_print_timings:       total time =    2521.39 ms /   924 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     155.39 ms /   256 runs   (    0.61 ms per token,  1647.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     236.59 ms /   685 tokens (    0.35 ms per token,  2895.29 tokens per second)\n",
      "llama_print_timings:        eval time =    3797.21 ms /   255 runs   (   14.89 ms per token,    67.15 tokens per second)\n",
      "llama_print_timings:       total time =    4322.86 ms /   940 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      96.98 ms /   166 runs   (    0.58 ms per token,  1711.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     308.11 ms /   990 tokens (    0.31 ms per token,  3213.12 tokens per second)\n",
      "llama_print_timings:        eval time =    2478.37 ms /   165 runs   (   15.02 ms per token,    66.58 tokens per second)\n",
      "llama_print_timings:       total time =    2964.38 ms /  1155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.17 ms /   256 runs   (    0.59 ms per token,  1693.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     168.19 ms /   515 tokens (    0.33 ms per token,  3062.01 tokens per second)\n",
      "llama_print_timings:        eval time =    3784.50 ms /   255 runs   (   14.84 ms per token,    67.38 tokens per second)\n",
      "llama_print_timings:       total time =    4237.84 ms /   770 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      62.19 ms /   105 runs   (    0.59 ms per token,  1688.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     139.27 ms /   463 tokens (    0.30 ms per token,  3324.43 tokens per second)\n",
      "llama_print_timings:        eval time =    1533.02 ms /   104 runs   (   14.74 ms per token,    67.84 tokens per second)\n",
      "llama_print_timings:       total time =    1780.22 ms /   567 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      83.64 ms /   141 runs   (    0.59 ms per token,  1685.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     309.95 ms /   998 tokens (    0.31 ms per token,  3219.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2105.01 ms /   140 runs   (   15.04 ms per token,    66.51 tokens per second)\n",
      "llama_print_timings:       total time =    2564.99 ms /  1138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      73.57 ms /   129 runs   (    0.57 ms per token,  1753.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     308.81 ms /   989 tokens (    0.31 ms per token,  3202.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1929.14 ms /   128 runs   (   15.07 ms per token,    66.35 tokens per second)\n",
      "llama_print_timings:       total time =    2374.38 ms /  1117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      74.11 ms /   127 runs   (    0.58 ms per token,  1713.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     222.95 ms /   619 tokens (    0.36 ms per token,  2776.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1868.08 ms /   126 runs   (   14.83 ms per token,    67.45 tokens per second)\n",
      "llama_print_timings:       total time =    2221.20 ms /   745 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      46.22 ms /    85 runs   (    0.54 ms per token,  1838.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     541.74 ms /  1594 tokens (    0.34 ms per token,  2942.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1266.53 ms /    84 runs   (   15.08 ms per token,    66.32 tokens per second)\n",
      "llama_print_timings:       total time =    1893.12 ms /  1678 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      56.59 ms /    95 runs   (    0.60 ms per token,  1678.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     332.86 ms /  1028 tokens (    0.32 ms per token,  3088.37 tokens per second)\n",
      "llama_print_timings:        eval time =    1410.45 ms /    94 runs   (   15.00 ms per token,    66.65 tokens per second)\n",
      "llama_print_timings:       total time =    1841.66 ms /  1122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      69.04 ms /   130 runs   (    0.53 ms per token,  1882.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     193.76 ms /   553 tokens (    0.35 ms per token,  2854.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1911.71 ms /   129 runs   (   14.82 ms per token,    67.48 tokens per second)\n",
      "llama_print_timings:       total time =    2232.58 ms /   682 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      63.04 ms /   108 runs   (    0.58 ms per token,  1713.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     218.49 ms /   589 tokens (    0.37 ms per token,  2695.74 tokens per second)\n",
      "llama_print_timings:        eval time =    1587.89 ms /   107 runs   (   14.84 ms per token,    67.39 tokens per second)\n",
      "llama_print_timings:       total time =    1918.07 ms /   696 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     145.21 ms /   256 runs   (    0.57 ms per token,  1762.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     296.74 ms /   905 tokens (    0.33 ms per token,  3049.78 tokens per second)\n",
      "llama_print_timings:        eval time =    3819.64 ms /   255 runs   (   14.98 ms per token,    66.76 tokens per second)\n",
      "llama_print_timings:       total time =    4395.41 ms /  1160 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      40.92 ms /    71 runs   (    0.58 ms per token,  1735.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     105.66 ms /   334 tokens (    0.32 ms per token,  3161.17 tokens per second)\n",
      "llama_print_timings:        eval time =    1015.16 ms /    70 runs   (   14.50 ms per token,    68.95 tokens per second)\n",
      "llama_print_timings:       total time =    1192.38 ms /   404 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     146.20 ms /   256 runs   (    0.57 ms per token,  1751.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.54 ms /   593 tokens (    0.37 ms per token,  2701.13 tokens per second)\n",
      "llama_print_timings:        eval time =    3789.36 ms /   255 runs   (   14.86 ms per token,    67.29 tokens per second)\n",
      "llama_print_timings:       total time =    4288.08 ms /   848 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     148.88 ms /   256 runs   (    0.58 ms per token,  1719.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     261.93 ms /   787 tokens (    0.33 ms per token,  3004.63 tokens per second)\n",
      "llama_print_timings:        eval time =    3805.23 ms /   255 runs   (   14.92 ms per token,    67.01 tokens per second)\n",
      "llama_print_timings:       total time =    4348.36 ms /  1042 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     148.06 ms /   256 runs   (    0.58 ms per token,  1729.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     401.59 ms /  1186 tokens (    0.34 ms per token,  2953.27 tokens per second)\n",
      "llama_print_timings:        eval time =    3834.61 ms /   255 runs   (   15.04 ms per token,    66.50 tokens per second)\n",
      "llama_print_timings:       total time =    4521.98 ms /  1441 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      54.25 ms /    89 runs   (    0.61 ms per token,  1640.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     104.39 ms /   321 tokens (    0.33 ms per token,  3074.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1273.03 ms /    88 runs   (   14.47 ms per token,    69.13 tokens per second)\n",
      "llama_print_timings:       total time =    1469.50 ms /   409 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     121.19 ms /   216 runs   (    0.56 ms per token,  1782.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     236.53 ms /   684 tokens (    0.35 ms per token,  2891.75 tokens per second)\n",
      "llama_print_timings:        eval time =    3194.56 ms /   215 runs   (   14.86 ms per token,    67.30 tokens per second)\n",
      "llama_print_timings:       total time =    3656.70 ms /   899 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     136.48 ms /   234 runs   (    0.58 ms per token,  1714.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.56 ms /   286 tokens (    0.35 ms per token,  2844.13 tokens per second)\n",
      "llama_print_timings:        eval time =    3368.77 ms /   233 runs   (   14.46 ms per token,    69.16 tokens per second)\n",
      "llama_print_timings:       total time =    3719.60 ms /   519 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.35 ms /   256 runs   (    0.59 ms per token,  1691.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     202.06 ms /   575 tokens (    0.35 ms per token,  2845.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3783.73 ms /   255 runs   (   14.84 ms per token,    67.39 tokens per second)\n",
      "llama_print_timings:       total time =    4269.08 ms /   830 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      64.26 ms /   110 runs   (    0.58 ms per token,  1711.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.09 ms /   883 tokens (    0.31 ms per token,  3209.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1630.02 ms /   109 runs   (   14.95 ms per token,    66.87 tokens per second)\n",
      "llama_print_timings:       total time =    2018.77 ms /   992 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.37 ms /   256 runs   (    0.58 ms per token,  1713.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     240.66 ms /   717 tokens (    0.34 ms per token,  2979.27 tokens per second)\n",
      "llama_print_timings:        eval time =    3796.10 ms /   255 runs   (   14.89 ms per token,    67.17 tokens per second)\n",
      "llama_print_timings:       total time =    4316.29 ms /   972 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     149.40 ms /   256 runs   (    0.58 ms per token,  1713.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.23 ms /   828 tokens (    0.32 ms per token,  3098.49 tokens per second)\n",
      "llama_print_timings:        eval time =    3808.30 ms /   255 runs   (   14.93 ms per token,    66.96 tokens per second)\n",
      "llama_print_timings:       total time =    4358.28 ms /  1083 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     113.72 ms /   208 runs   (    0.55 ms per token,  1829.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     109.05 ms /   370 tokens (    0.29 ms per token,  3392.88 tokens per second)\n",
      "llama_print_timings:        eval time =    3017.50 ms /   207 runs   (   14.58 ms per token,    68.60 tokens per second)\n",
      "llama_print_timings:       total time =    3338.50 ms /   577 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      54.25 ms /    92 runs   (    0.59 ms per token,  1695.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     138.85 ms /   467 tokens (    0.30 ms per token,  3363.37 tokens per second)\n",
      "llama_print_timings:        eval time =    1338.05 ms /    91 runs   (   14.70 ms per token,    68.01 tokens per second)\n",
      "llama_print_timings:       total time =    1569.91 ms /   558 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      57.49 ms /    96 runs   (    0.60 ms per token,  1669.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     129.42 ms /   391 tokens (    0.33 ms per token,  3021.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1380.22 ms /    95 runs   (   14.53 ms per token,    68.83 tokens per second)\n",
      "llama_print_timings:       total time =    1609.11 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     138.83 ms /   241 runs   (    0.58 ms per token,  1735.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     298.86 ms /   925 tokens (    0.32 ms per token,  3095.11 tokens per second)\n",
      "llama_print_timings:        eval time =    3600.28 ms /   240 runs   (   15.00 ms per token,    66.66 tokens per second)\n",
      "llama_print_timings:       total time =    4162.99 ms /  1165 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     148.10 ms /   256 runs   (    0.58 ms per token,  1728.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.21 ms /   290 tokens (    0.35 ms per token,  2865.41 tokens per second)\n",
      "llama_print_timings:        eval time =    3709.06 ms /   255 runs   (   14.55 ms per token,    68.75 tokens per second)\n",
      "llama_print_timings:       total time =    4092.95 ms /   545 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      42.27 ms /    73 runs   (    0.58 ms per token,  1727.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     111.77 ms /   379 tokens (    0.29 ms per token,  3390.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1043.97 ms /    72 runs   (   14.50 ms per token,    68.97 tokens per second)\n",
      "llama_print_timings:       total time =    1228.00 ms /   451 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     107.12 ms /   178 runs   (    0.60 ms per token,  1661.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     140.62 ms /   484 tokens (    0.29 ms per token,  3442.00 tokens per second)\n",
      "llama_print_timings:        eval time =    2617.25 ms /   177 runs   (   14.79 ms per token,    67.63 tokens per second)\n",
      "llama_print_timings:       total time =    2949.19 ms /   661 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =     151.93 ms /   256 runs   (    0.59 ms per token,  1684.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     240.98 ms /   716 tokens (    0.34 ms per token,  2971.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3803.76 ms /   255 runs   (   14.92 ms per token,    67.04 tokens per second)\n",
      "llama_print_timings:       total time =    4333.19 ms /   971 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7109.20 ms\n",
      "llama_print_timings:      sample time =      47.88 ms /    79 runs   (    0.61 ms per token,  1649.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     101.56 ms /   290 tokens (    0.35 ms per token,  2855.43 tokens per second)\n",
      "llama_print_timings:        eval time =    1128.90 ms /    78 runs   (   14.47 ms per token,    69.09 tokens per second)\n",
      "llama_print_timings:       total time =    1311.86 ms /   368 tokens\n"
     ]
    }
   ],
   "source": [
    "qa_kb = []\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import jsonlines\n",
    "for idx, question in enumerate(tqdm(question_as_list)):\n",
    "    q_temp = copy.deepcopy(template_q)\n",
    "    q_temp[\"content\"] = question\n",
    "    a_temp = copy.deepcopy(template_a)\n",
    "    response = query_engine.query(question)\n",
    "    a_temp[\"content\"] = str(response)\n",
    "    \n",
    "    qa_kb.append({\"conversations\": [q_temp, a_temp], \"system\": \"You are a helpful assistant who is helping a customer with questions about Farcaster.\"})\n",
    "    with jsonlines.open(f'../qa/loop_temp/demo_data_out_{idx:03}.jsonl', 'w') as writer:\n",
    "        writer.write_all(qa_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7cf896e0-9684-4b48-980d-e4c034564b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with jsonlines.open('../qa/demo_data.jsonl', 'r') as r:\n",
    "#     data_demo = [each_line for each_line in r]\n",
    "# out = data_demo + qa_kb\n",
    "\n",
    "# with jsonlines.open('../qa/demo_data_out3.jsonl', 'w') as writer:\n",
    "#     writer.write_all(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1c7abdc-00c9-4237-8825-cf7ccb7602e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../qa/demo_data_out2.jsonl', 'r') as r:\n",
    "    data_demo = [each_line for each_line in r]\n",
    "out = data_demo + qa_kb\n",
    "\n",
    "with jsonlines.open('../qa/demo_data_out2_1.jsonl', 'w') as writer:\n",
    "    writer.write_all(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42d6ef-4dec-4f73-bb8c-6adc4b426169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
