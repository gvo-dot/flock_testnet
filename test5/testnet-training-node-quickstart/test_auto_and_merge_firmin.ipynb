{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d190978-d7bc-4b30-a1cd-18384868fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TASK_ID=\"5\"\n",
    "FLOCK_API_KEY=\"E1D0YPIGPWKKWS4AAI6ROZHZ1PDJFH9R\" \n",
    "HF_TOKEN=\"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\" \n",
    "CUDA_VISIBLE_DEVICES=\"0\"\n",
    "HF_USERNAME=\"yifanxie\"\n",
    "\n",
    "os.environ[\"TASK_ID\"] = TASK_ID\n",
    "os.environ[\"FLOCK_API_KEY\"] = FLOCK_API_KEY\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA_VISIBLE_DEVICES\n",
    "os.environ[\"HF_USERNAME\"] = HF_USERNAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40a89d94-8574-4416-99e3-49f204ba2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import requests\n",
    "import yaml\n",
    "from loguru import logger\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "from demo import LoraTrainingArguments, train_lora\n",
    "from utils.constants import model2base_model, model2size\n",
    "from utils.flock_api import get_task, submit_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e11e7c5-d4b9-401d-81d7-043b0bd6c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = os.environ[\"TASK_ID\"]\n",
    "# load trainin args\n",
    "# define the path of the current file\n",
    "\n",
    "with open(f\"training_args-firmin.yaml\", \"r\") as f:\n",
    "    all_training_args_as_list = yaml.safe_load(f)\n",
    "\n",
    "task = get_task(task_id)\n",
    "# log the task info\n",
    "# logger.info(json.dumps(task, indent=4))\n",
    "# download data from a presigned url\n",
    "data_url = task[\"data\"][\"training_set_url\"]\n",
    "context_length = task[\"data\"][\"context_length\"]\n",
    "max_params = task[\"data\"][\"max_params\"]\n",
    "\n",
    "# filter out the model within the max_params\n",
    "#model2size = {k: v for k, v in model2size.items() if v <= max_params}\n",
    "#all_training_args = {k: v for k, v in all_training_args.items() if k in model2size}\n",
    "#logger.info(f\"Models within the max_params: {all_training_args.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ffe9432-0def-47df-ad46-60cdb3ab5920",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_repo_id_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9709c9e5-62ab-4bd2-ad30-8b930ef246ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 20:21:39.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mStart to train the model Qwen/Qwen1.5-0.5B...\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-06-12 20:21:41.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoading data: demo_data.jsonl\u001b[0m\n",
      "\u001b[32m2024-06-12 20:21:41.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mThere are 282 data in dataset\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 01:23, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.234500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-12 20:23:07.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStart to push the lora weight to the hub...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c750ae5407f4815bef1fc01c12f0b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21dff3bdd1484184baa1e9f15840ac17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/3.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e83097b78194ceabe129cc14829f182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 20:23:10.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mTask submitted successfully\u001b[0m\n",
      "\u001b[32m2024-06-12 20:23:11.000\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mStart to train the model Qwen/Qwen1.5-0.5B...\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-06-12 20:23:13.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoading data: demo_data.jsonl\u001b[0m\n",
      "\u001b[32m2024-06-12 20:23:13.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mThere are 282 data in dataset\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.508400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-12 20:23:56.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStart to push the lora weight to the hub...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c0a4a4e94945eca7e8a5cd0ac016c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3679d904dfd0435e9385eeb1ecba0c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c001fb49d347d7904725db92b2cb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 20:24:00.065\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[31m\u001b[1mError: Failed to submit task: {\"detail\":\"Wallet has reached the daily rate limit for task submissions\"}\u001b[0m\n",
      "\u001b[32m2024-06-12 20:24:00.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mProceed to the next model...\u001b[0m\n",
      "\u001b[32m2024-06-12 20:24:00.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mStart to train the model Qwen/Qwen1.5-0.5B...\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-06-12 20:24:02.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoading data: demo_data.jsonl\u001b[0m\n",
      "\u001b[32m2024-06-12 20:24:02.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mThere are 282 data in dataset\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.509700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-12 20:24:46.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStart to push the lora weight to the hub...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ee828afd92419cb6b2d824b456d86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.98M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3660ab96a5f47d5a899d20b928eadbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17161e022d44010af3741475eb2de69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 20:24:49.573\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m38\u001b[0m - \u001b[31m\u001b[1mError: Failed to submit task: {\"detail\":\"Wallet has reached the daily rate limit for task submissions\"}\u001b[0m\n",
      "\u001b[32m2024-06-12 20:24:49.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mProceed to the next model...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for all_training_args in all_training_args_as_list:\n",
    "    model_id = list(all_training_args.keys())[0]\n",
    "    logger.info(f\"Start to train the model {model_id}...\")\n",
    "    # if OOM, proceed to the next model\n",
    "    try:\n",
    "        train_lora(\n",
    "            model_id=model_id,\n",
    "            context_length=context_length,\n",
    "            training_args=LoraTrainingArguments(**all_training_args[model_id]),\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Error: {e}\")\n",
    "        logger.info(\"Proceed to the next model...\")\n",
    "        continue\n",
    "\n",
    "    # generate a random repo id based on timestamp\n",
    "    hg_repo_id = f\"{model_id.replace('/', '-')}-\" + str(int(time.time()))\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Start to push the lora weight to the hub...\")\n",
    "        api = HfApi(token=os.environ[\"HF_TOKEN\"])\n",
    "        api.create_repo(\n",
    "            f\"{HF_USERNAME}/{hg_repo_id}\",\n",
    "            repo_type=\"model\",\n",
    "        )\n",
    "        api.upload_folder(\n",
    "            folder_path=\"outputs\",\n",
    "            repo_id=f\"{HF_USERNAME}/{hg_repo_id}\",\n",
    "            repo_type=\"model\",\n",
    "        )\n",
    "        # submit\n",
    "        #submit_task(\n",
    "        #    task_id, f\"{HF_USERNAME}/{hg_repo_id}\", model2base_model[model_id]\n",
    "        #)\n",
    "        hg_repo_id_list.append(f\"{HF_USERNAME}/{hg_repo_id}\")\n",
    "        logger.info(\"Task submitted successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}\")\n",
    "        logger.info(\"Proceed to the next model...\")\n",
    "    finally:\n",
    "        # cleanup merged_model and output\n",
    "        os.system(\"rm -rf merged_model\")\n",
    "        os.system(\"rm -rf outputs\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c90a2ba-8979-4b4e-b422-9189f94de4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_repo_id_list=[\n",
    "'yifanxie/Qwen-Qwen1.5-0.5B-1718223886',\n",
    "'yifanxie/Qwen-Qwen1.5-0.5B-1718223836',\n",
    "'yifanxie/Qwen-Qwen1.5-0.5B-1718223787']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9529ded5-b195-4085-a01a-c88b71badf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 21:14:04.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mStart to train the model Qwen/Qwen1.5-0.5B...\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-06-12 21:14:07.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoading data: demo_data.jsonl\u001b[0m\n",
      "\u001b[32m2024-06-12 21:14:07.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mThere are 282 data in dataset\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 01:23, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.239500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-12 21:15:32.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mStart to train the model Qwen/Qwen1.5-0.5B...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-06-12 21:15:35.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoading data: demo_data.jsonl\u001b[0m\n",
      "\u001b[32m2024-06-12 21:15:35.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mThere are 282 data in dataset\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:40, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.508800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-12 21:16:18.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mStart to train the model Qwen/Qwen1.5-0.5B...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-06-12 21:16:20.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mLoading data: demo_data.jsonl\u001b[0m\n",
      "\u001b[32m2024-06-12 21:16:20.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mThere are 282 data in dataset\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.509400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "from peft_demo import LoraTrainingArguments, train_lora_peft\n",
    "for all_training_args in all_training_args_as_list:\n",
    "    model_id = list(all_training_args.keys())[0]\n",
    "    logger.info(f\"Start to train the model {model_id}...\")\n",
    "    # if OOM, proceed to the next model\n",
    "    try:\n",
    "        train_lora_peft(\n",
    "            model_id=model_id,\n",
    "            context_length=context_length,\n",
    "            training_args=LoraTrainingArguments(**all_training_args[model_id]),\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Error: {e}\")\n",
    "        logger.info(\"Proceed to the next model...\")\n",
    "        continue\n",
    "\n",
    "    # generate a random repo id based on timestamp\n",
    "    hg_repo_id = f\"{model_id.replace('/', '-')}-\" + str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "131eb0cf-2414-4c82-a221-f0be7f5b7f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,838,401,536 || trainable%: 0.0856\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "model_name_or_path = \"Qwen/Qwen1.5-1.8B\"\n",
    "tokenizer_name_or_path = \"Qwen/Qwen1.5-1.8B\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "853dd0f6-dd5e-4afa-9a2c-a8a0577ce0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"rm -rf outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b2933-1320-441b-91fa-ac3853ce9975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
