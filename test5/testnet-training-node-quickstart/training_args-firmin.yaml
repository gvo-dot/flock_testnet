- Qwen/Qwen1.5-0.5B:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 8
    num_train_epochs: 2
    lora_rank: 8
    lora_alpha: 16
    lora_dropout: 0.1
- Qwen/Qwen1.5-0.5B:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 8
    num_train_epochs: 1
    lora_rank: 4
    lora_alpha: 8
    lora_dropout: 0.2
- Qwen/Qwen1.5-0.5B:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 8
    num_train_epochs: 1
    lora_rank: 5
    lora_alpha: 8
    lora_dropout: 0.3